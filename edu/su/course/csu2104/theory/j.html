<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>C5 - CSU2104 - Shoolini U</title>
        <meta name="description" content="C5 - CSU2104 - Shoolini U">

        <meta property=" og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <meta name="author" content="Divya Mohan">
        <meta name="robots" content="index, follow">

        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script> -->

        <!-- <style>
            main ul {
                list-style-type: none;
                padding: 0;
                margin: 0;
            }

            main ul li {
                margin: 0;
                padding: 0;
            }
        </style> -->

        <!-- JSON-LD Structured Data for SEO -->
        <script type="application/ld+json">
            {
              "@context": "https://schema.org",
              "@type": "Course",
              "name": "Topic Name",
              "description": "Topic description,
              "provider": {
                "@type": "EducationalOrganization",
                "name": "Shoolini University",
                "url": "https://shooliniuniversity.com"
              }
            }
        </script>


    </head>

    <body>

        <script> header_author("dm"); </script>

        <main>
            <article class="agen-tableofcontents">
                <h2 class="text-center">
                    C5 (C3)
                </h2>
                <div class="d-none contentdate">2025, April 09</div>
            </article>

<pre id="markdown-raw">
**Random Forest**

---

### 1. What is it?

Random Forest is an **ensemble learning** method used for classification and regression. It builds a “forest” of many decision trees during training and outputs the class (classification) or mean prediction (regression) of the individual trees. By combining multiple trees, it reduces overfitting and improves generalization compared to a single decision tree.

---

### 2. Example

Imagine you want to classify whether an email is “spam” or “not spam.”

1. You collect a dataset of labeled emails (features might include word counts, sender domain, presence of links, etc.).
2. Random Forest creates, say, 100 different decision trees—each trained on a random subset of emails and a random subset of features at each split.
3. When a new email arrives, each tree votes “spam” or “not spam”; the forest’s final decision is the majority vote.

---

### 3. Advantages & Disadvantages

| Advantages                                                                                                       | Disadvantages                                                                              |
| ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| • **High accuracy**: Aggregates many trees to reduce variance.                                                   | • **Less interpretable**: Hard to visualize or explain individual decisions.               |
| • **Robust to overfitting**: Randomness in samples and features prevents trees from all learning the same noise. | • **Computational cost**: Training and prediction can be slow with hundreds of deep trees. |
| • **Handles high-dimensional data** well (many features).                                                        | • **Memory usage**: Storing many trees consumes more memory.                               |
| • **Built-in feature importance**: Provides relative scores for each feature.                                    | • **May struggle with very sparse data** if patterns are subtle.                           |

---

### 4. Related Subconcepts

* **Decision Tree**: The base learner; splits data recursively on feature thresholds.
* **Bagging (Bootstrap Aggregation)**: Training each tree on a bootstrap sample (random sampling with replacement).
* **Feature Subsampling**: At each node, only a random subset of features is considered for splitting.
* **Out-of-Bag (OOB) Error**: An internal error estimate computed by using samples not included in each tree’s bootstrap.
* **Gini Impurity & Entropy**: Metrics to choose the best splits in classification trees.
* **Ensemble Methods**: Techniques that combine multiple models to improve performance (e.g., boosting, stacking).

---

### 5. Overview – How Does It Work?

1. **Bootstrap Sampling**

   * From the original dataset of size *N*, sample *N* points *with replacement* to form the training set for each tree.
2. **Random Feature Selection**

   * At each node split, pick a random subset of *m* features (out of all *M*) and choose the best split only among those.
3. **Tree Growth**

   * Grow each decision tree fully (or until a stopping criterion like minimum leaf size).
4. **Aggregation**

   * **Classification**: Each tree votes for a class; the forest predicts the majority.
   * **Regression**: Take the average of all trees’ numerical outputs.
5. **Error Estimation & Feature Importance**

   * Compute OOB error by predicting each sample with only the trees that did **not** include it in their bootstrap.
   * Measure feature importance by how much each feature decreases impurity across all trees.

---

**Decision Tree**

---

**1. What is it?**
A decision tree is a non-parametric supervised learning algorithm used for classification and regression. It represents decisions and their possible outcomes in a tree structure, where internal nodes denote tests on features, branches represent test outcomes, and leaf nodes correspond to predicted class labels or continuous values.

---

**2. Example**
Consider a credit‐approval system that determines whether to grant a loan.

* **Features**: Applicant’s income, credit history score, outstanding debts.
* **Process**:

  1. At the root node, test “credit history ≥ 700?”.
  2. If yes, proceed to test “income ≥ \$50 000?”; if no, test “outstanding debts ≤ \$5 000?”.
  3. Continue splitting until leaf nodes predict “Approve” or “Reject.”

---

**3. Advantages and Disadvantages**

| Advantages                                                                     | Disadvantages                                                         |
| ------------------------------------------------------------------------------ | --------------------------------------------------------------------- |
| • Interpretable model: Decision paths can be visualized and explained.         | • Prone to overfitting if grown without constraints.                  |
| • Handles both numerical and categorical data without extensive preprocessing. | • Small changes in data may produce a different tree (high variance). |
| • Requires little feature scaling or normalization.                            | • Can create biased trees if some classes dominate.                   |
| • Fast inference once trained.                                                 | • Splitting criteria may favor features with more levels.             |

---

**4. Related Subconcepts**

* **Splitting Criteria**: Gini impurity, information gain (entropy), variance reduction.
* **Tree Induction Algorithms**: ID3, C4.5, CART.
* **Pruning**: Pre-pruning (stop growth early) and post-pruning (remove branches) to control overfitting.
* **Leaf Node Assignment**: Majority class for classification; average target value for regression.
* **Ensemble Extensions**: Boosting, bagging, and random forests build upon individual decision trees.

---

**5. Overview – How Does It Work?**

1. **Select Root Feature**: Evaluate all features using a splitting criterion and choose the feature that best separates the target classes or minimizes error.
2. **Partition Data**: Divide the dataset into subsets according to the selected feature’s values or threshold.
3. **Recursive Splitting**: For each subset, repeat feature selection and partitioning until a stopping condition is met (e.g., maximum depth, minimum samples per node, or no further impurity reduction).
4. **Create Leaf Nodes**: Assign class labels (classification) or compute mean values (regression) for terminal nodes.
5. **Prediction**: For a new instance, traverse the tree from the root, following branches corresponding to feature values, until reaching a leaf node that provides the predicted output.

**Brute Force**

---

**1. What is it?**
Brute force is an exhaustive search strategy that systematically enumerates all possible candidates for a solution and checks each one until a valid solution is found or all possibilities are exhausted. It makes no assumptions or optimizations about the problem structure.

---

**2. Example**
To solve the subset sum problem—determining if any subset of a given set of integers sums to a target value—brute force would generate every possible subset (2ⁿ subsets for n elements) and compute each subset’s sum until it matches the target.

---

**3. Advantages and Disadvantages**

| Advantages                                                   | Disadvantages                                                           |
| ------------------------------------------------------------ | ----------------------------------------------------------------------- |
| • Guaranteed to find a solution if one exists.               | • Exponential time complexity; impractical for large problem sizes.     |
| • Simple to implement; minimal algorithmic insight required. | • High computational and memory cost due to full enumeration.           |
| • Easily parallelizable by dividing the search space.        | • No pruning of unpromising candidates unless combined with heuristics. |

---

**4. Related Subconcepts**

* **Search Space**: The set of all candidate solutions.
* **Time Complexity**: Often O(kⁿ) or O(n!), depending on problem.
* **Pruning Techniques**: Methods (e.g., branch-and-bound) to skip parts of the search space.
* **Heuristic Search**: Incorporating domain knowledge to guide or reduce enumeration.
* **Dynamic Programming**: Replaces brute-force by storing and reusing intermediate results.

---

**5. Overview – How Does It Work??**

1. **Define Candidate Representation**: Determine how to encode each possible solution.
2. **Enumerate Candidates**: Generate every candidate in the search space (e.g., via recursion, binary counters).
3. **Evaluate Each Candidate**: For each, compute a cost or check a validity condition.
4. **Select Valid/Optimal Solutions**: Collect or return the first valid solution (decision problems) or the best according to the evaluation metric (optimization problems).
5. **Terminate**: Stop when a solution is found (if only one is needed) or after examining all candidates to ensure optimality.

**Greedy Algorithms**

---

**1. What is it?**
A greedy algorithm is a problem-solving paradigm that builds a solution piece by piece, always choosing the next piece that offers the most immediate benefit. At each step it makes the locally optimal choice, without revisiting previous decisions, aiming to reach a globally optimal solution.

---

**2. Example**
**Activity Selection**
Given a set of activities with start and finish times, select the maximum number of non-overlapping activities.

1. Sort all activities by their finish times in ascending order.
2. Pick the first activity (earliest finish).
3. For each subsequent activity, if its start time is ≥ finish time of the last selected activity, select it.
   This yields an optimal schedule in O(n log n) time.

---

**3. Advantages and Disadvantages**

| Advantages                                               | Disadvantages                                                            |
| -------------------------------------------------------- | ------------------------------------------------------------------------ |
| • **Simplicity**: Easy to understand and implement.      | • **No guarantee** of global optimum for all problems.                   |
| • **Efficiency**: Often runs in O(n log n) or O(n) time. | • **Requires proof** of greedy-choice property and optimal substructure. |
| • **Low overhead**: Minimal bookkeeping and memory use.  | • **Limited applicability** to problems satisfying specific properties.  |

---

**4. Related Subconcepts**

* **Greedy-Choice Property**: A globally optimal solution can be arrived at by making locally optimal (greedy) choices.
* **Optimal Substructure**: An optimal solution to the problem contains optimal solutions to its subproblems.
* **Exchange Argument**: A proof technique demonstrating that any optimal solution can be transformed into the greedy solution by exchanging choices.
* **Matroids**: Abstract structures generalizing the conditions under which greedy algorithms yield optimal solutions.
* **Classic Greedy Algorithms**:

  * **Huffman Coding**: Builds an optimal prefix code by repeatedly merging least-frequent symbols.
  * **Prim’s and Kruskal’s MST**: Constructs minimum spanning tree by selecting smallest edges without creating cycles.
  * **Dijkstra’s Shortest Path**: Selects the next closest vertex to the source, updating path estimates.

---

**5. Overview – How Does It Work?**

1. **Characterize Candidates**: Identify all possible choices (e.g., activities, edges, symbols).
2. **Define Selection Function**: Establish a rule to evaluate and pick the best candidate at each step (e.g., earliest finish time, smallest weight).
3. **Order Candidates**: If necessary, sort candidates by the selection criterion.
4. **Iterate and Select**:

   * While unvisited candidates remain and solution is incomplete:

     * Choose the best available candidate.
     * Add it to the solution if it remains feasible.
     * Update the set of candidates or constraints.
5. **Terminate**: When no more candidates can be added, return the constructed solution.


**Linear Regression**

---

**1. What is it?**
A statistical method for modeling the relationship between a continuous dependent variable and one or more independent variables by fitting a linear equation to observed data.

---

**2. Example**
Predicting a student’s final exam score based on hours of study:

* Dependent variable (*y*): exam score.
* Independent variable (*x*): hours studied.
* Model fit: *y* = *β₀* + *β₁x*.

---

**3. Advantages and Disadvantages**

| Advantages                                                                     | Disadvantages                                                        |
| ------------------------------------------------------------------------------ | -------------------------------------------------------------------- |
| • Interpretability: Coefficients quantify feature impact.                      | • Linearity assumption: Relationship must be linear.                 |
| • Low computational cost: Closed-form solution via normal equations.           | • Sensitive to outliers: Extreme values distort the fit.             |
| • Well-studied inference: Confidence intervals and hypothesis tests available. | • Cannot model complex, nonlinear patterns without transformation.   |
| • Efficient for large datasets with few features.                              | • Multicollinearity among predictors inflates variance of estimates. |

---

**4. Related Subconcepts**

* **Ordinary Least Squares (OLS):** Minimizes sum of squared residuals to estimate coefficients.
* **Gradient Descent:** Iterative optimization for large or high-dimensional datasets.
* **Regularization:** Ridge (L2) and Lasso (L1) penalties to prevent overfitting.
* **R² and Adjusted R²:** Measures of explained variance.
* **Assumptions:** Linearity, independence, homoscedasticity, normality of errors.

---

**5. Overview – How Does It Work?**

1. **Formulate Model:** Assume *yᵢ* = *β₀* + Σᵢ(*βⱼ xᵢⱼ*) + *εᵢ*.
2. **Estimate Coefficients:**

   * OLS: *β* = (XᵀX)⁻¹Xᵀy.
   * Or use gradient descent: iteratively update *β* ← *β* – α·∇₍β₎MSE.
3. **Evaluate Fit:** Compute residuals (*y* – ŷ), then R².
4. **Validate Assumptions:** Plot residuals vs. fitted values; conduct tests for heteroscedasticity and normality.
5. **Prediction:** For new *x*, compute ŷ = *β₀* + Σ(*βⱼ xⱼ*).

---

**Logistic Regression**

---

**1. What is it?**
A classification algorithm that models the probability of a binary outcome using the logistic function to map linear combinations of inputs to the interval (0, 1).

---

**2. Example**
Predicting whether an email is spam (1) or not spam (0) based on features such as number of links and frequency of certain keywords. The model estimates:

$$
P(\text{spam}=1|\mathbf{x}) = \frac{1}{1 + e^{- (\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n)}}
$$

---

**3. Advantages and Disadvantages**

| Advantages                                                             | Disadvantages                                                           |
| ---------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| • Probabilistic output: Provides class probabilities.                  | • Assumes linear decision boundary in feature space.                    |
| • Efficient to train: Convex loss function ensures global minimum.     | • May underperform when the true boundary is highly nonlinear.          |
| • Well understood regularization: L1/L2 penalties prevent overfitting. | • Sensitive to multicollinearity among predictors.                      |
| • Interpretable coefficients: Log-odds interpretation.                 | • Requires careful feature scaling for convergence in gradient descent. |

---

**4. Related Subconcepts**

* **Logit Function:** *logit(p) = ln(p/(1–p))*, linear in parameters.
* **Maximum Likelihood Estimation (MLE):** Finds coefficients that maximize the likelihood of observed labels.
* **Regularization:** L1 (Lasso) and L2 (Ridge) to shrink coefficients.
* **ROC Curve & AUC:** Evaluate classification performance across thresholds.
* **Multinomial and Ordinal Extensions:** For multi-class or ordered outcomes.

---

**5. Overview – How Does It Work?**

1. **Model Specification:** Define *η* = *β₀* + Σ(*βⱼ xⱼ*), then link via logistic: *p* = 1/(1+e⁻ᶻ).
2. **Parameter Estimation:**

   * Use MLE: maximize *L(β)* = Πᵢ pᵢʸⁱ (1–pᵢ)^(1–ʸⁱ).
   * Typically solved via iterative methods (e.g., Newton-Raphson).
3. **Model Assessment:**

   * Compute confusion matrix, accuracy, precision, recall.
   * Plot ROC curve; calculate AUC.
4. **Regularization (if needed):** Add penalty term to log-likelihood:

   * L1: λΣ|βⱼ|; L2: λΣβⱼ².
5. **Prediction:**

   * For new *x*, compute *p*.
   * Assign class = 1 if *p* ≥ threshold (commonly 0.5), otherwise 0.
   

**Knapsack Algorithm**

---

**1. What is it?**
The knapsack problem is a classic combinatorial optimization problem: given a set of items, each with a weight and a value, determine the subset of items to include in a “knapsack” of fixed capacity so as to maximize total value. The most common formulation is the 0/1 knapsack, in which each item may be chosen at most once. A dynamic programming algorithm solves it in pseudo-polynomial time.

---

**2. Example**

* **Items**:

  1. Weight = 2, Value = 3
  2. Weight = 3, Value = 4
  3. Weight = 4, Value = 5
  4. Weight = 5, Value = 6
* **Capacity**: 8
* **Optimal Selection (0/1 knapsack)**: Items 2 and 4 (weights 3+5=8) for total value 4+6=10.

---

**3. Advantages and Disadvantages**

| Advantages                                                                  | Disadvantages                                                                        |
| --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| • **Optimality**: Dynamic programming guarantees the best solution.         | • **Pseudo-polynomial time**: O(n·C), where C is capacity—impractical if C is large. |
| • **Exact solution for integer weights**: No approximation error.           | • **High memory use**: O(n·C) table can exhaust resources.                           |
| • **Extensible**: Variants (bounded, unbounded) handled with minor changes. | • **Not suited for real-valued weights** without discretization.                     |

---

**4. Related Subconcepts**

* **Dynamic Programming**: Tabulation (bottom-up) vs. memoization (top-down).
* **Greedy Fractional Knapsack**: Sort by value/weight ratio, optimal only when fractional items allowed.
* **Branch and Bound**: Prunes branches in a search tree to accelerate exact 0/1 solutions.
* **Approximation Schemes**: FPTAS provides (1–ε)-optimal solutions in polynomial time.
* **Complexity Classes**: NP-hardness of general integer knapsack; pseudo-polynomial solvability.

---

**5. Overview – How Does It Work?**

1. **Table Construction**

   * Create a 2D table `dp[i][w]`, for `i=0…n` items and `w=0…C` capacity.
   * Initialize `dp[0][*] = 0` and `dp[*][0] = 0`.
2. **Recursive Relation**

   * For each item `i` (1 to n) and each weight `w` (1 to C):

     ```
     if weight[i] ≤ w:
       dp[i][w] = max(dp[i-1][w],
                      dp[i-1][w - weight[i]] + value[i])
     else:
       dp[i][w] = dp[i-1][w]
     ```
3. **Result Extraction**

   * The maximum achievable value is `dp[n][C]`.
   * Backtrack from `dp[n][C]` to identify which items were included.
4. **Fractional Variant (Greedy)**

   * Sort items by `(value/weight)` descending.
   * Fill knapsack by taking whole items until the next item exceeds remaining capacity, then take a fraction.
5. **Extensions & Optimizations**

   * **Space Optimization**: Use a 1D array of length C, updating in reverse weight order.
   * **Approximation**: Scale values or weights to reduce table size for an FPTAS.


</pre>


        </main>


        <script> copyright("all"); </script>

    </body>

</html>