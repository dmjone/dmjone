<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>Voice Biometric System - CSU1530 - Shoolini U</title>
        <meta name="description" content="Learn about the theory and implementation of voice biometric systems in the context of CSU1530 at Shoolini University.">

        <meta property="og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <meta name="author" content="Divya Mohan">
        <meta name="robots" content="index, follow">

        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script>

    </head>

    <body>

        <script> header_author("dm"); </script>

        <main>
            <article class="agen-tableofcontents">
                <h2 class="text-center">
                    Voice Biometric System
                </h2>
                <div class="d-none contentdate">2024, September 25</div>
            </article>

            <article>
                <h3>1. Introduction to Voice Biometric Systems</h3>
                <p>Voice Biometric Systems identify or verify individuals based on the unique characteristics of their vocal patterns. These systems analyze features such as pitch, tone, and speech dynamics to authenticate users. Voice biometrics offer a convenient and natural way for security and authentication in various applications.</p>
                <p>Applications include:</p>
                <ul>
                    <li><strong>Access Control</strong>: Securing entry to facilities or devices through voice verification.</li>
                    <li><strong>Telecommunication Services</strong>: Authenticating users in call centers or phone banking.</li>
                    <li><strong>Forensic Analysis</strong>: Assisting in criminal investigations through voice identification.</li>
                </ul>
            </article>

            <article>
                <h3>2. Characteristics of Human Voice</h3>
                <p>The human voice contains features that are unique to each individual, making it suitable for biometric recognition.</p>
            </article>

            <article>
                <h4>2.1 Physiological Features</h4>
                <p>Attributes related to the physical structure of the vocal tract:</p>
                <ul>
                    <li><strong>Vocal Tract Shape</strong>: The configuration of the mouth, throat, and nasal passages affects voice production.</li>
                    <li><strong>Vocal Cord Characteristics</strong>: The length and tension of vocal cords influence pitch and timbre.</li>
                </ul>
            </article>

            <article>
                <h4>2.2 Behavioral Features</h4>
                <p>Attributes related to speaking habits and patterns:</p>
                <ul>
                    <li><strong>Pronunciation</strong>: Individual ways of articulating words and sounds.</li>
                    <li><strong>Speaking Rhythm</strong>: Unique patterns in speech timing and pauses.</li>
                    <li><strong>Accent and Dialect</strong>: Regional or cultural influences on speech.</li>
                </ul>
            </article>

            <article>
                <h3>3. Voice Data Acquisition</h3>
                <p>Capturing high-quality voice recordings is essential for accurate recognition.</p>
            </article>

            <article>
                <h4>3.1 Acquisition Methods</h4>
                <p>Techniques for collecting voice data include:</p>
                <ul>
                    <li><strong>Microphones</strong>: Standard devices for recording speech in various environments.</li>
                    <li><strong>Telephone Networks</strong>: Capturing voice over telecommunication systems.</li>
                    <li><strong>Mobile Devices</strong>: Using built-in microphones in smartphones and tablets.</li>
                </ul>
            </article>

            <article>
                <h4>3.2 Challenges in Acquisition</h4>
                <p>Potential issues during voice data capture:</p>
                <ul>
                    <li><strong>Background Noise</strong>: Environmental sounds can interfere with voice signals.</li>
                    <li><strong>Variability in Recording Devices</strong>: Different microphones have varying sensitivities and qualities.</li>
                    <li><strong>Channel Effects</strong>: Transmission over networks can introduce distortions.</li>
                </ul>
                <p>Mitigation strategies include noise reduction techniques and consistent recording setups.</p>
            </article>

            <article>
                <h3>4. Preprocessing of Voice Signals</h3>
                <p>Preprocessing enhances voice recordings and prepares them for feature extraction.</p>
            </article>

            <article>
                <h4>4.1 Noise Reduction</h4>
                <p>Removing unwanted sounds from the voice signal:</p>
                <ul>
                    <li><strong>Filtering</strong>: Applying low-pass or band-pass filters to isolate the frequency range of human speech.</li>
                    <li><strong>Spectral Subtraction</strong>: Estimating and subtracting the noise spectrum from the signal.</li>
                </ul>
            </article>

            <article>
                <h4>4.2 Voice Activity Detection</h4>
                <p>Identifying segments of the recording that contain speech:</p>
                <ul>
                    <li><strong>Energy-Based Methods</strong>: Detecting speech based on signal energy thresholds.</li>
                    <li><strong>Statistical Models</strong>: Using probabilistic methods to distinguish speech from silence or noise.</li>
                </ul>
            </article>

            <article>
                <h4>4.3 Normalization</h4>
                <p>Standardizing the voice signal for consistent analysis:</p>
                <ul>
                    <li><strong>Amplitude Normalization</strong>: Adjusting signal levels to a common amplitude.</li>
                    <li><strong>Time Normalization</strong>: Aligning speech signals in time, especially for dynamic features.</li>
                </ul>
            </article>

            <article>
                <h3>5. Feature Extraction in Voice Biometrics</h3>
                <p>Extracting distinctive features from the voice signal to create a representative feature vector.</p>
            </article>

            <article>
                <h4>5.1 Short-Term Spectral Features</h4>
                <p>Analyzing the frequency content of short segments of the voice signal:</p>
                <ul>
                    <li><strong>Mel-Frequency Cepstral Coefficients (MFCC)</strong>: Capturing the spectral properties of speech in a perceptually meaningful way.</li>
                    <li><strong>Linear Predictive Coding (LPC)</strong>: Modeling the vocal tract to represent the speech signal.</li>
                </ul>
                <p>MFCC calculation steps:</p>
                <ol>
                    <li>Divide the signal into overlapping frames.</li>
                    <li>Apply a window function (e.g., Hamming window) to each frame.</li>
                    <li>Compute the Fast Fourier Transform (FFT) of each frame.</li>
                    <li>Map the powers of the spectrum onto the mel scale using triangular filter banks.</li>
                    <li>Take the logarithm of the filter bank energies.</li>
                    <li>Compute the Discrete Cosine Transform (DCT) of the log energies.</li>
                </ol>
                <p>MFCCs are the resulting coefficients from the DCT.</p>
            </article>

            <article>
                <h4>5.2 Prosodic Features</h4>
                <p>Capturing long-term characteristics of speech:</p>
                <ul>
                    <li><strong>Pitch (Fundamental Frequency)</strong>: The perceived frequency of the voice.</li>
                    <li><strong>Intensity</strong>: The loudness of speech over time.</li>
                    <li><strong>Speaking Rate</strong>: The speed at which an individual speaks.</li>
                </ul>
            </article>

            <article>
                <h4>5.3 Spectral Dynamics</h4>
                <p>Analyzing changes in the spectral content over time:</p>
                <ul>
                    <li><strong>Delta and Delta-Delta Coefficients</strong>: First and second-order time derivatives of features like MFCCs.</li>
                </ul>
                <p>Delta coefficients are computed as:</p>
                <p>$$ \Delta c_t = \frac{\sum_{n=1}^N n (c_{t+n} - c_{t-n})}{2 \sum_{n=1}^N n^2} $$</p>
                <ul>
                    <li><strong>\( c_t \)</strong>: Feature coefficient at time \( t \).</li>
                    <li><strong>\( N \)</strong>: Number of frames for computing the derivative.</li>
                </ul>
            </article>

            <article>
                <h3>6. Matching and Classification</h3>
                <p>Comparing voice features to identify or verify individuals.</p>
            </article>

            <article>
                <h4>6.1 Distance Metrics</h4>
                <p>Calculating similarity between feature vectors using:</p>
                <ul>
                    <li><strong>Euclidean Distance</strong>: Measures the straight-line distance between vectors.</li>
                    <li><strong>Cosine Similarity</strong>: Computes the cosine of the angle between vectors.</li>
                    <li><strong>Kullback-Leibler Divergence</strong>: Measures the difference between probability distributions.</li>
                </ul>
            </article>

            <article>
                <h4>6.2 Classification Algorithms</h4>
                <p>Methods for assigning voice data to identities:</p>
                <ul>
                    <li><strong>Gaussian Mixture Models (GMM)</strong>: Modeling the probability distribution of features for each individual.</li>
                    <li><strong>Support Vector Machines (SVM)</strong>: Finding the optimal separating hyperplane between classes.</li>
                    <li><strong>Deep Neural Networks (DNN)</strong>: Learning complex representations through multiple layers.</li>
                </ul>
            </article>

            <article>
                <h4>6.2 Speaker Modeling with GMM</h4>
                <p>Creating a model for each speaker using GMMs:</p>
                <ul>
                    <li>Estimate the parameters \( \theta = \{ w_i, \mu_i, \Sigma_i \} \) of the GMM.</li>
                    <li><strong>\( w_i \)</strong>: Mixture weights.</li>
                    <li><strong>\( \mu_i \)</strong>: Mean vectors.</li>
                    <li><strong>\( \Sigma_i \)</strong>: Covariance matrices.</li>
                </ul>
                <p>The likelihood of a feature vector \( \mathbf{x} \) is:</p>
                <p>$$ p(\mathbf{x}|\theta) = \sum_{i=1}^M w_i \mathcal{N}(\mathbf{x}|\mu_i, \Sigma_i) $$</p>
                <ul>
                    <li><strong>\( M \)</strong>: Number of mixtures.</li>
                    <li><strong>\( \mathcal{N} \)</strong>: Multivariate Gaussian distribution.</li>
                </ul>
            </article>

            <article>
                <h3>7. Evaluation Metrics</h3>
                <p>Assessing the performance of voice biometric systems using statistical measures.</p>
            </article>

            <article>
                <h4>7.1 Equal Error Rate (EER)</h4>
                <p>The point where the false acceptance rate equals the false rejection rate.</p>
                <p>A lower EER indicates better system performance.</p>
            </article>

            <article>
                <h4>7.2 Detection Error Trade-off (DET) Curve</h4>
                <p>Plots false rejection rate against false acceptance rate on a normal deviate scale.</p>
                <p>Helps in visualizing and comparing system performance.</p>
            </article>

            <article>
                <h4>7.3 Receiver Operating Characteristic (ROC) Curve</h4>
                <p>Plots true positive rate against false positive rate at various thresholds.</p>
                <p>Provides insights into the trade-offs between detection and false alarm rates.</p>
            </article>

            <article>
                <h3>8. Challenges in Voice Biometrics</h3>
                <p>Factors that can affect the accuracy and reliability of voice biometric systems.</p>
            </article>

            <article>
                <h4>8.1 Variability in Speech</h4>
                <p>Differences in voice due to various factors:</p>
                <ul>
                    <li><strong>Emotional State</strong>: Stress or excitement can alter voice characteristics.</li>
                    <li><strong>Health Conditions</strong>: Illnesses affecting the throat or nasal passages.</li>
                    <li><strong>Aging</strong>: Changes in vocal cords over time.</li>
                </ul>
                <p>Mitigation strategies include updating voice models and using robust features.</p>
            </article>

            <article>
                <h4>8.2 Environmental Noise</h4>
                <p>Background sounds can interfere with voice signals.</p>
                <p>Approaches:</p>
                <ul>
                    <li><strong>Noise Cancellation</strong>: Using algorithms to reduce background noise.</li>
                    <li><strong>Robust Feature Extraction</strong>: Focusing on features less sensitive to noise.</li>
                </ul>
            </article>

            <article>
                <h4>8.3 Channel Variability</h4>
                <p>Differences in recording devices and transmission channels.</p>
                <p>Solutions:</p>
                <ul>
                    <li><strong>Channel Compensation Techniques</strong>: Normalizing effects of different channels.</li>
                    <li><strong>Use of Universal Background Models (UBM)</strong>: Modeling common characteristics across speakers.</li>
                </ul>
            </article>

            <article>
                <h4>8.4 Spoofing Attacks</h4>
                <p>Attempts to deceive the system using recorded or synthetic voices.</p>
                <p>Countermeasures:</p>
                <ul>
                    <li><strong>Liveness Detection</strong>: Identifying signs of a live human speaker.</li>
                    <li><strong>Anti-Spoofing Algorithms</strong>: Detecting artifacts in synthesized or replayed audio.</li>
                </ul>
            </article>

            <article>
                <h3>9. Implementation Example</h3>
                <p>An example of building a voice biometric system using MFCC for feature extraction and GMM for classification.</p>
            </article>

            <article>
                <h4>9.1 Data Preparation</h4>
                <p>Steps involved:</p>
                <ol>
                    <li><strong>Collect Voice Samples</strong>: Gather recordings from multiple speakers with labels.</li>
                    <li><strong>Preprocess Recordings</strong>:
                        <ul>
                            <li>Apply noise reduction techniques.</li>
                            <li>Perform voice activity detection to isolate speech segments.</li>
                        </ul>
                    </li>
                </ol>
            </article>

            <article>
                <h4>9.2 Feature Extraction with MFCC</h4>
                <p>Extracting MFCC features from voice samples.</p>
                <pre><code class="language-python">import numpy as np
import librosa

def extract_mfcc_features(signal, sample_rate, num_coefficients):
    # Compute MFCCs
    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_coefficients)
    # Transpose to get time frames as rows
    mfccs = mfccs.T
    return mfccs

# Example usage
signal, sample_rate = librosa.load('voice_sample.wav', sr=None)
num_coefficients = 13
mfcc_features = extract_mfcc_features(signal, sample_rate, num_coefficients)
</code></pre>
                <p>Include delta and delta-delta coefficients for capturing dynamics.</p>
            </article>

            <article>
                <h4>9.3 Speaker Modeling with GMM</h4>
                <p>Training a GMM for each speaker.</p>
                <pre><code class="language-python">from sklearn.mixture import GaussianMixture

def train_gmm(features, num_components):
    # Create and train GMM
    gmm = GaussianMixture(n_components=num_components, covariance_type='diag', max_iter=200)
    gmm.fit(features)
    return gmm

# Example usage
num_components = 16
speaker_models = {}
for speaker_id, features in speaker_features.items():
    gmm = train_gmm(features, num_components)
    speaker_models[speaker_id] = gmm
</code></pre>
            </article>

            <article>
                <h4>9.4 Recognition of New Voice Samples</h4>
                <p>Identifying the speaker of a new voice sample.</p>
                <pre><code class="language-python">def recognize_speaker(mfcc_features, speaker_models):
    scores = {}
    for speaker_id, gmm in speaker_models.items():
        # Compute log-likelihood
        log_likelihood = gmm.score(mfcc_features)
        scores[speaker_id] = log_likelihood
    # Identify the speaker with the highest score
    identified_speaker = max(scores, key=scores.get)
    return identified_speaker

# Example usage
new_signal, new_sample_rate = librosa.load('new_voice_sample.wav', sr=None)
new_mfcc_features = extract_mfcc_features(new_signal, new_sample_rate, num_coefficients)
predicted_speaker = recognize_speaker(new_mfcc_features, speaker_models)
print(f'Identified Speaker: {predicted_speaker}')
</code></pre>
            </article>

            <article>
                <h4>9.5 Evaluating the System</h4>
                <p>Assessing system performance using test samples.</p>
                <pre><code class="language-python"># Test the recognition function
correct = 0
total = len(test_samples)
for true_speaker, sample_path in test_samples.items():
    signal, sample_rate = librosa.load(sample_path, sr=None)
    mfcc_features = extract_mfcc_features(signal, sample_rate, num_coefficients)
    predicted_speaker = recognize_speaker(mfcc_features, speaker_models)
    if predicted_speaker == true_speaker:
        correct += 1

accuracy = correct / total * 100
print(f'Accuracy: {accuracy:.2f}%')
</code></pre>
            </article>

            <article>
                <h3>10. Summary</h3>
                <p>Voice Biometric Systems leverage the unique characteristics of an individual's voice for identification and verification. By understanding the processes of voice data acquisition, preprocessing, feature extraction, and classification, effective voice recognition applications can be developed. Addressing challenges such as variability in speech and environmental noise is crucial for enhancing system performance and reliability.</p>
            </article>


        </main>

        <script> copyright("all"); </script>

    </body>

</html>