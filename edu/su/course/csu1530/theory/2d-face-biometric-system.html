<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>2D Face Biometric System - CSU1530 - Shoolini U</title>
        <meta name="description" content="Learn about the 2D Face Biometric System in CSU1530 at Shoolini University">

        <meta property="og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <meta name="author" content="Divya Mohan">
        <meta name="robots" content="index, follow">

        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script> -->

    </head>

    <body>

        <script> header_author("dm"); </script>

        <main>
            <article class="agen-tableofcontents">
                <h2 class="text-center">
                    2D Face Biometric System
                </h2>
                <div class="d-none contentdate">2024, September 5</div>
            </article>

            <article>
                <h3>1. Introduction to 2D Face Biometric Systems</h3>
                <p>2D Face Biometric Systems are technologies that identify or verify individuals by analyzing facial features from two-dimensional images. These systems are widely used due to their non-intrusive nature and ease of deployment with existing camera infrastructure.</p>
                <p>Key applications include:</p>
                <ul>
                    <li><strong>Security and Surveillance</strong>: Monitoring public spaces for known threats.</li>
                    <li><strong>Access Control</strong>: Granting entry to secure facilities or devices.</li>
                    <li><strong>Authentication</strong>: Verifying identities for financial transactions or user logins.</li>
                </ul>
                <p>The core processes involve <strong>face detection</strong>, <strong>feature extraction</strong>, and <strong>face recognition</strong>.</p>
            </article>

            <article>
                <h3>2. Face Detection</h3>
                <p>Face detection is the process of locating human faces in digital images. It's crucial as it isolates the facial region for further processing.</p>
            </article>

            <article>
                <h4>2.1 Haar Cascades</h4>
                <p>Developed by Viola and Jones, Haar Cascades use machine learning to identify facial features using simple rectangular patterns called Haar-like features.</p>
                <p>Key characteristics:</p>
                <ul>
                    <li><strong>Speed</strong>: Real-time detection capability.</li>
                    <li><strong>Training</strong>: Requires many positive and negative images to train the classifier.</li>
                    <li><strong>Application</strong>: Commonly used in OpenCV for face detection tasks.</li>
                </ul>
                <p>Example code:</p>
                <pre><code class="language-python">import cv2

# Load pre-trained Haar Cascade classifier
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Read image and convert to grayscale
image = cv2.imread('test_image.jpg')
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Detect faces
faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.3, minNeighbors=5)

# Draw rectangles around faces
for (x, y, w, h) in faces:
    cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)
</code></pre>
            </article>

            <article>
                <h4>2.2 Histogram of Oriented Gradients (HOG)</h4>
                <p>HOG descriptors capture the distribution of gradient orientations in localized portions of an image.</p>
                <p>Advantages:</p>
                <ul>
                    <li><strong>Robustness</strong>: Effective in varying lighting conditions.</li>
                    <li><strong>Descriptor</strong>: Provides a feature vector representing the image.</li>
                    <li><strong>Usage</strong>: Often combined with Support Vector Machines (SVM) for classification.</li>
                </ul>
                <p>Processing steps:</p>
                <ol>
                    <li>Divide the image into small connected regions called cells.</li>
                    <li>Compute gradient orientation and magnitude for each pixel.</li>
                    <li>Create a histogram of gradient orientations for each cell.</li>
                    <li>Normalize across larger regions called blocks to reduce illumination effects.</li>
                </ol>
            </article>

            <article>
                <h3>3. Feature Extraction</h3>
                <p>Feature extraction transforms facial images into a set of numerical values (features) that capture essential characteristics for recognition.</p>
            </article>

            <article>
                <h4>3.1 Principal Component Analysis (PCA)</h4>
                <p>PCA is a statistical method that reduces the dimensionality of data while preserving as much variance as possible.</p>
                <p>Applications in face recognition:</p>
                <ul>
                    <li><strong>Dimensionality Reduction</strong>: Converts high-resolution images into a lower-dimensional space.</li>
                    <li><strong>Eigenfaces</strong>: The principal components are called eigenfaces when applied to face images.</li>
                </ul>
                <p>Mathematical formulation:</p>
                <p>Given a centered data matrix \( X \in \mathbb{R}^{n \times d} \), where \( n \) is the number of samples and \( d \) is the dimensionality, PCA solves the eigenvalue problem:</p>
                <p>$$ X^T X v = \lambda v $$</p>
                <p>Where:</p>
                <ul>
                    <li><strong>\( v \)</strong>: Eigenvectors representing principal components.</li>
                    <li><strong>\( \lambda \)</strong>: Corresponding eigenvalues indicating the amount of variance captured.</li>
                </ul>
            </article>

            <article>
                <h4>3.2 Linear Discriminant Analysis (LDA)</h4>
                <p>LDA seeks to find a linear combination of features that best separates two or more classes.</p>
                <p>Benefits over PCA:</p>
                <ul>
                    <li><strong>Class Discrimination</strong>: Focuses on maximizing class separability.</li>
                    <li><strong>Reduced Dimensionality</strong>: The maximum number of discriminant components is \( c - 1 \), where \( c \) is the number of classes.</li>
                </ul>
                <p>Mathematical objective:</p>
                <p>Maximize the Fisher criterion:</p>
                <p>$$ J(w) = \frac{w^T S_B w}{w^T S_W w} $$</p>
                <p>Where:</p>
                <ul>
                    <li><strong>\( S_B \)</strong>: Between-class scatter matrix.</li>
                    <li><strong>\( S_W \)</strong>: Within-class scatter matrix.</li>
                    <li><strong>\( w \)</strong>: Projection vector.</li>
                </ul>
            </article>

            <article>
                <h3>4. Face Recognition Algorithms</h3>
                <p>Face recognition involves comparing the extracted features to a database to identify or verify an individual.</p>
            </article>

            <article>
                <h4>4.1 Eigenfaces</h4>
                <p>The Eigenfaces method represents face images as linear combinations of principal components derived from PCA.</p>
                <p>Process:</p>
                <ol>
                    <li>Compute the mean face and subtract it from all face images.</li>
                    <li>Perform PCA to find eigenvectors (eigenfaces).</li>
                    <li>Project new face images onto the eigenface space.</li>
                    <li>Compare projections using distance metrics like Euclidean distance.</li>
                </ol>
                <p>Face reconstruction formula:</p>
                <p>$$ \Phi_{\text{reconstructed}} = \overline{\Phi} + \sum_{i=1}^k w_i u_i $$</p>
                <ul>
                    <li><strong>\( \Phi_{\text{reconstructed}} \)</strong>: Reconstructed face image.</li>
                    <li><strong>\( \overline{\Phi} \)</strong>: Mean face image.</li>
                    <li><strong>\( w_i \)</strong>: Weights (projections) onto eigenfaces.</li>
                    <li><strong>\( u_i \)</strong>: Eigenfaces.</li>
                </ul>
            </article>

            <article>
                <h4>4.2 Fisherfaces</h4>
                <p>Fisherfaces combine PCA and LDA to improve recognition performance under varying lighting and facial expressions.</p>
                <p>Advantages:</p>
                <ul>
                    <li><strong>Enhanced Discrimination</strong>: LDA focuses on class separability.</li>
                    <li><strong>Robustness</strong>: Better performance with changes in illumination and expression.</li>
                </ul>
                <p>Implementation steps:</p>
                <ol>
                    <li>Apply PCA to reduce dimensionality and eliminate noise.</li>
                    <li>Apply LDA on PCA-transformed data to maximize class separability.</li>
                </ol>
            </article>

            <article>
                <h4>4.3 Local Binary Patterns (LBP)</h4>
                <p>LBP is a texture descriptor that labels pixels based on the local neighborhood.</p>
                <p>How it works:</p>
                <ol>
                    <li>For each pixel, compare it to its surrounding neighbors.</li>
                    <li>Assign a binary value: 1 if the neighbor pixel value is greater or equal, 0 otherwise.</li>
                    <li>Concatenate the binary values to form a binary number (the LBP code).</li>
                    <li>Use histograms of LBP codes as feature vectors.</li>
                </ol>
                <p>Features:</p>
                <ul>
                    <li><strong>Computational Efficiency</strong>: Simple calculations suitable for real-time applications.</li>
                    <li><strong>Rotation Invariance</strong>: Can be extended to be rotation-invariant.</li>
                    <li><strong>Illumination Insensitivity</strong>: Robust against monotonic gray-scale changes.</li>
                </ul>
            </article>

            <article>
                <h3>5. Matching and Classification</h3>
                <p>After feature extraction, the next step is to compare the features of the input face with those in the database.</p>
            </article>

            <article>
                <h4>5.1 Distance Metrics</h4>
                <p>Commonly used distance metrics include:</p>
                <ul>
                    <li><strong>Euclidean Distance</strong>: Measures the straight-line distance between two points in feature space.</li>
                    <li><strong>Cosine Similarity</strong>: Measures the cosine of the angle between two vectors, indicating orientation rather than magnitude.</li>
                    <li><strong>Mahalanobis Distance</strong>: Considers the correlations between variables, useful when features are not independent.</li>
                </ul>
            </article>

            <article>
                <h4>5.2 Classification Algorithms</h4>
                <p>Algorithms used for classifying face features:</p>
                <ul>
                    <li><strong>Nearest Neighbor (NN)</strong>: Assigns the class of the closest training sample.</li>
                    <li><strong>Support Vector Machines (SVM)</strong>: Finds the hyperplane that best separates classes in feature space.</li>
                    <li><strong>Neural Networks</strong>: Learns complex patterns through multiple layers (e.g., Deep Learning models).</li>
                </ul>
            </article>

            <article>
                <h3>6. Evaluation Metrics</h3>
                <p>Evaluating the performance of face recognition systems is crucial to understand their effectiveness.</p>
            </article>

            <article>
                <h4>6.1 False Acceptance Rate (FAR)</h4>
                <p>FAR quantifies the likelihood that the system incorrectly accepts an unauthorized individual.</p>
                <p>Formula:</p>
                <p>$$ \text{FAR} = \frac{\text{Number of False Acceptances}}{\text{Total Number of Impostor Attempts}} $$</p>
                <p>A lower FAR indicates better security against unauthorized access.</p>
            </article>

            <article>
                <h4>6.2 False Rejection Rate (FRR)</h4>
                <p>FRR measures the probability that the system incorrectly rejects an authorized individual.</p>
                <p>Formula:</p>
                <p>$$ \text{FRR} = \frac{\text{Number of False Rejections}}{\text{Total Number of Genuine Attempts}} $$</p>
                <p>A lower FRR indicates better user convenience and accessibility.</p>
            </article>

            <article>
                <h4>6.3 Receiver Operating Characteristic (ROC) Curve</h4>
                <p>The ROC curve plots the trade-off between FAR and FRR across different threshold settings.</p>
                <p>Characteristics:</p>
                <ul>
                    <li><strong>Area Under Curve (AUC)</strong>: A higher AUC indicates better overall performance.</li>
                    <li><strong>Equal Error Rate (EER)</strong>: The point where FAR equals FRR; lower EER signifies better accuracy.</li>
                </ul>
                <p>Interpreting ROC curves helps in selecting an optimal threshold balancing security and usability.</p>
            </article>

            <article>
                <h3>7. Challenges in 2D Face Biometrics</h3>
                <p>Despite advancements, 2D face biometric systems face several challenges that can affect accuracy.</p>
            </article>

            <article>
                <h4>7.1 Illumination Variations</h4>
                <p>Lighting changes can significantly alter the appearance of facial features.</p>
                <p>Mitigation techniques:</p>
                <ul>
                    <li><strong>Histogram Equalization</strong>: Enhances contrast by spreading out intensity values.</li>
                    <li><strong>Illumination Normalization</strong>: Methods like Retinex algorithms adjust images to a standard lighting condition.</li>
                    <li><strong>Use of Invariant Features</strong>: Extract features less sensitive to lighting, such as LBP.</li>
                </ul>
            </article>

            <article>
                <h4>7.2 Pose Variations</h4>
                <p>Faces captured at different angles can cause misalignment and recognition errors.</p>
                <p>Solutions:</p>
                <ul>
                    <li><strong>3D Face Models</strong>: Use 3D representations to account for pose changes.</li>
                    <li><strong>Multi-view Training</strong>: Include images from various angles in the training dataset.</li>
                    <li><strong>Pose Estimation</strong>: Detect and normalize the pose before recognition.</li>
                </ul>
            </article>

            <article>
                <h4>7.3 Expression Variations</h4>
                <p>Facial expressions can alter the geometry of facial features.</p>
                <p>Approaches to handle expressions:</p>
                <ul>
                    <li><strong>Expression-Invariant Features</strong>: Focus on areas less affected by expressions, like the periocular region.</li>
                    <li><strong>Dynamic Models</strong>: Use models that capture facial muscle movements.</li>
                    <li><strong>Augmented Training Data</strong>: Incorporate various expressions in the training set.</li>
                </ul>
            </article>

            <article>
                <h4>7.4 Occlusions</h4>
                <p>Obstructions like glasses, masks, or hair can hide facial features.</p>
                <p>Strategies to overcome occlusions:</p>
                <ul>
                    <li><strong>Partial Matching</strong>: Match visible parts of the face independently.</li>
                    <li><strong>Occlusion Detection</strong>: Identify and ignore occluded regions during recognition.</li>
                    <li><strong>Robust Feature Extraction</strong>: Use features less susceptible to occlusions.</li>
                </ul>
            </article>

            <article>
                <h3>8. Implementation Example</h3>
                <p>An example of face recognition using PCA and Eigenfaces is provided below.</p>
            </article>

            <article>
                <h4>8.1 Data Preparation</h4>
                <p>Steps:</p>
                <ol>
                    <li><strong>Collect Face Images</strong>: Gather a dataset of face images with labels.</li>
                    <li><strong>Preprocess Images</strong>:
                        <ul>
                            <li>Convert to grayscale.</li>
                            <li>Normalize image sizes.</li>
                            <li>Align faces if necessary.</li>
                        </ul>
                    </li>
                    <li><strong>Flatten Images</strong>: Convert 2D images into 1D vectors.</li>
                </ol>
            </article>

            <article>
                <h4>8.2 PCA Implementation</h4>
                <p>Using Scikit-learn's PCA module:</p>
                <pre><code class="language-python">import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Assuming X is the data matrix with flattened images
# and y contains corresponding labels

# Standardize data
scaler = StandardScaler().fit(X)
X_std = scaler.transform(X)

# Apply PCA
pca = PCA(n_components=100, whiten=True)
X_pca = pca.fit_transform(X_std)
</code></pre>
                <p>Notes:</p>
                <ul>
                    <li><strong>Whitening</strong>: Decorrelates the components and scales them to unit variance.</li>
                    <li><strong>Number of Components</strong>: Choose \( k \) such that a significant amount of variance (e.g., 95%) is retained.</li>
                </ul>
            </article>

            <article>
                <h4>8.3 Classification with Nearest Neighbor</h4>
                <p>Using the transformed data for recognition:</p>
                <pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Train Nearest Neighbor classifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

# Evaluate on test set
accuracy = knn.score(X_test, y_test)
print(f'Accuracy: {accuracy * 100:.2f}%')
</code></pre>
                <p>Interpretation:</p>
                <ul>
                    <li><strong>Accuracy</strong>: Percentage of correct predictions on the test set.</li>
                    <li><strong>Overfitting</strong>: Using more components than necessary may lead to overfitting.</li>
                </ul>
            </article>

            <article>
                <h4>8.4 Predicting New Faces</h4>
                <p>To recognize a new face:</p>
                <pre><code class="language-python"># Load and preprocess new face image
new_face = load_new_face_image()
new_face_flat = new_face.flatten()
new_face_std = scaler.transform([new_face_flat])

# Project onto PCA components
new_face_pca = pca.transform(new_face_std)

# Predict using trained classifier
prediction = knn.predict(new_face_pca)
print(f'Identified as: {prediction[0]}')
</code></pre>
                <p>Ensure that the new face image undergoes the same preprocessing steps as the training data.</p>
            </article>

            <article>
                <h3>9. Conclusion</h3>
                <p>Understanding 2D Face Biometric Systems involves grasping the entire pipeline from face detection to recognition and evaluation. With algorithms like PCA, LDA, and feature descriptors like LBP, one gains insights into how facial recognition systems work and the challenges they face. Implementing these systems requires careful consideration of factors like illumination, pose, and expression to achieve robust and accurate recognition.</p>
            </article>


        </main>

        <script> copyright("all"); </script>

    </body>

</html>