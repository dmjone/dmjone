<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>Iris Biometric System - CSU1530 - Shoolini U</title>
        <meta name="description" content="Learn about the Iris Biometric System in which the iris of the eye is used for identification and authentication.">

        <meta property="og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <meta name="author" content="Divya Mohan">
        <meta name="robots" content="index, follow">

        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script> -->

    </head>

    <body>

        <script> header_author("dm"); </script>

        <main>
            <article class="agen-tableofcontents">
                <h2 class="text-center">
                    Iris Biometric System
                </h2>
                <div class="d-none contentdate">2024, August 20</div>
            </article>

            <article>
                <h3>1. Introduction to Iris Biometric System</h3>
                <p>Biometric systems are used to authenticate or identify individuals based on their unique physical or behavioral traits. Among various biometric modalities, iris recognition has emerged as one of the most reliable and accurate forms of identification. The human iris, the colored part of the eye, contains rich, unique patterns that remain stable over time, making it an ideal characteristic for biometric recognition. Unlike fingerprints, which can wear out over time, or facial features, which can change, the iris is protected behind the cornea, ensuring its stability and longevity.</p>
                <p>In an iris biometric system, high-resolution images of an individual's iris are captured and processed to extract distinctive features that are used to create a template. This template is stored securely in a database and compared with newly captured iris images for authentication or identification purposes. The system offers a high level of accuracy due to the complexity and uniqueness of the iris patterns, which are different even between identical twins.</p>
                <p>Iris biometric systems are widely used in high-security applications, such as border control, access to secure facilities, and national identification systems, due to their resistance to forgery and spoofing attacks. The following sections explore the various components of an iris biometric system, starting with the acquisition model.</p>
            </article>

            <article>
                <h3>2. Acquisition Model in Iris Biometric System</h3>
                <p>Iris biometric systems rely on the unique patterns found in the human eye’s iris for identity verification. The iris is a thin, colored ring of tissue that surrounds the pupil and has unique patterns that can be captured and analyzed for recognition. Iris biometric systems offer high accuracy and security due to the rich texture and stable patterns of the iris.</p>
            </article>

            <article>
                <h4>2.1 Acquisition Model</h4>
                <p>The acquisition process in iris biometric systems involves capturing a high-quality image of the iris. This process is critical to ensure accurate identification or verification, as poor quality images can lead to errors. The acquisition model consists of several steps that must be followed to collect a usable iris image.</p>
            </article>

            <article>
                <h5>2.1.1 Imaging Sensors</h5>
                <p>The first step in the acquisition process is the use of specialized imaging sensors. These sensors are designed to capture high-resolution images of the iris without being obstructed by eyelids, eyelashes, or reflections from the cornea. Near-infrared (NIR) light is commonly used to illuminate the iris, as it reduces the interference from reflections and enhances the visibility of iris patterns.</p>
                <ul>
                    <li><strong>NIR illumination</strong>: Ensures proper visibility of the iris patterns even under low lighting conditions.</li>
                    <li><strong>High-resolution sensors</strong>: Capture detailed images of the iris, typically with a resolution of 640x480 pixels or higher.</li>
                    <li><strong>Image quality assessment</strong>: Ensures that the captured image is free from noise, occlusion, and blurriness.</li>
                </ul>
            </article>

            <article>
                <h5>2.1.2 Pupil and Iris Segmentation</h5>
                <p>After capturing the iris image, the system segments the region of interest, which includes isolating the iris from the surrounding features like the pupil, sclera, and eyelids. This segmentation ensures that only the iris patterns are analyzed.</p>
                <ul>
                    <li><strong>Pupil detection</strong>: Locates the center and boundary of the pupil, which is important for normalization of the iris image.</li>
                    <li><strong>Iris boundary detection</strong>: Detects the outer boundary of the iris to separate it from the sclera and other surrounding tissues.</li>
                    <li><strong>Eyelid/eyelash occlusion detection</strong>: Identifies areas where eyelids or eyelashes might obstruct the iris, ensuring they are excluded from the analysis.</li>
                </ul>
                <pre><code class="language-python">
# Example of image segmentation
def segment_iris(image):
    pupil_boundary = detect_pupil(image)
    iris_boundary = detect_iris(image)
    return extract_iris_region(image, pupil_boundary, iris_boundary)
</code></pre>
            </article>

            <article>
                <h5>2.1.3 Image Normalization</h5>
                <p>To account for variations in the size of the iris due to different distances between the camera and the eye, the system normalizes the iris image. This step involves adjusting the captured image to a fixed size and orientation, allowing the system to standardize the iris patterns for easier comparison.</p>
                <p>The most common normalization technique is converting the segmented iris into a fixed rectangular format using the Daugman rubber sheet model.</p>
                <pre><code class="language-python">
# Example of normalization using the rubber sheet model
def normalize_iris(iris_image, pupil_center, iris_boundary):
    return rubber_sheet_model(iris_image, pupil_center, iris_boundary)
</code></pre>
            </article>

            <article>
                <h5>2.1.4 Feature Extraction</h5>
                <p>After normalization, the system extracts the unique patterns in the iris, such as furrows, ridges, and crypts. These features are mathematically represented and stored as an iris template.</p>
                <p>Commonly used methods for feature extraction include Gabor wavelet transform, which captures the texture and frequency information of the iris patterns.</p>
                <ul>
                    <li><strong>Gabor filters</strong>: Used to extract texture information from the iris image at different orientations and frequencies.</li>
                    <li><strong>Feature encoding</strong>: Converts the extracted features into a binary code, called an iris code, that uniquely represents the iris.</li>
                </ul>
                <pre><code class="language-python">
# Example of feature extraction
def extract_features(iris_image):
    return apply_gabor_filter(iris_image)
</code></pre>
            </article>

            <article>
                <h5>2.1.5 Iris Template Creation and Matching</h5>
                <p>Once the features are extracted, an iris template is created, which is a compact binary representation of the unique iris patterns. This template is compared with the stored templates in the database for identification or verification.</p>
                <p>Matching is performed using Hamming distance, which calculates the number of differing bits between two iris codes.</p>
                <pre><code class="language-python">
# Example of iris code matching using Hamming distance
def match_iris(iris_code_1, iris_code_2):
    return hamming_distance(iris_code_1, iris_code_2)
</code></pre>
                <ul>
                    <li><strong>Template storage</strong>: Iris templates are stored in the database and encrypted for security purposes.</li>
                    <li><strong>Matching threshold</strong>: A predefined threshold value is used to determine whether two iris codes match (i.e., belong to the same individual).</li>
                </ul>
            </article>

            <article>
                <h3>3. Segmentation in Iris Biometric Systems</h3>
                <p>Segmentation is a critical step in iris biometric systems, as it focuses on isolating the iris region from other parts of the eye such as the pupil, sclera, eyelids, and eyelashes. Accurate segmentation is essential because any misalignment or inclusion of unwanted regions can drastically affect the recognition performance. The process involves detecting the boundaries of the iris and excluding irrelevant areas.</p>
            </article>

            <article>
                <h4>3.1 Segmentation Process</h4>
                <p>The segmentation process typically consists of several stages, each aiming to identify the exact boundaries of the iris and handle obstructions such as eyelids and eyelashes. This section will cover each stage in detail.</p>
            </article>

            <article>
                <h5>3.1.1 Pupil Detection</h5>
                <p>Pupil detection is the first step in the segmentation process. The pupil is generally darker than the surrounding iris, making it easier to identify using image processing techniques. Once the pupil is detected, its center and radius can be calculated, which helps in isolating the iris.</p>
                <p>One commonly used algorithm for pupil detection is the Circular Hough Transform, which detects circular shapes in the image.</p>
                <pre><code class="language-python">
# Example of pupil detection using Circular Hough Transform
def detect_pupil(iris_image):
    return hough_circular_transform(iris_image, min_radius, max_radius)
</code></pre>
                <ul>
                    <li><strong>Pupil boundary</strong>: The detected circular boundary of the pupil helps define the inner limit of the iris.</li>
                    <li><strong>Center of the pupil</strong>: Important for normalizing the iris image in later steps.</li>
                </ul>
            </article>

            <article>
                <h5>3.1.2 Iris Boundary Detection</h5>
                <p>After detecting the pupil, the next step is to locate the outer boundary of the iris, which separates the iris from the sclera (white part of the eye). Similar to pupil detection, the Circular Hough Transform can be applied to detect this boundary.</p>
                <p>Once both the pupil and iris boundaries are detected, the region between these two boundaries is considered the iris region for further processing.</p>
                <pre><code class="language-python">
# Example of iris boundary detection
def detect_iris_boundary(iris_image, pupil_center, pupil_radius):
    return hough_circular_transform(iris_image, pupil_center, pupil_radius, min_radius, max_radius)
</code></pre>
                <ul>
                    <li><strong>Iris boundary</strong>: Defines the outer limit of the iris, separating it from the sclera.</li>
                    <li><strong>Iris region</strong>: The area between the detected pupil and iris boundaries is isolated for feature extraction.</li>
                </ul>
            </article>

            <article>
                <h5>3.1.3 Noise Removal</h5>
                <p>During segmentation, noise such as reflections, shadows, and occlusions from eyelids or eyelashes can interfere with the accurate detection of the iris. These obstructions need to be handled carefully to ensure clean segmentation.</p>
                <p>Noise removal techniques may involve thresholding, edge detection, and morphological operations to filter out unwanted areas of the image. Reflections, which often appear as bright spots, can be removed by detecting their high-intensity pixel values.</p>
                <pre><code class="language-python">
# Example of noise removal using thresholding
def remove_noise(iris_image):
    return apply_thresholding(iris_image)
</code></pre>
                <ul>
                    <li><strong>Reflection removal</strong>: Detects and removes bright reflections on the cornea that could interfere with iris detection.</li>
                    <li><strong>Eyelash/eyelid occlusion handling</strong>: Detects and excludes areas covered by eyelids or eyelashes.</li>
                </ul>
            </article>

            <article>
                <h5>3.1.4 Eyelid and Eyelash Detection</h5>
                <p>Eyelids and eyelashes may partially or fully occlude the iris, which can lead to inaccurate recognition. Detecting these occlusions is necessary to exclude those regions from further analysis.</p>
                <p>The upper and lower eyelid curves can be modeled using parabolic curve fitting, while individual eyelashes can be detected by analyzing the intensity and texture of the image. Typically, the areas occluded by eyelashes or eyelids are masked out.</p>
                <pre><code class="language-python">
# Example of eyelid detection using parabolic curve fitting
def detect_eyelids(iris_image):
    return parabolic_curve_fitting(iris_image)
</code></pre>
                <ul>
                    <li><strong>Eyelid exclusion</strong>: Detects the curve of the eyelid and masks the area it occludes.</li>
                    <li><strong>Eyelash exclusion</strong>: Identifies individual eyelashes and masks their regions to avoid interference with iris recognition.</li>
                </ul>
            </article>

            <article>
                <h5>3.1.5 Iris Masking</h5>
                <p>Once the pupil, iris boundaries, and occlusions are detected, the next step is to create a mask that isolates only the valid iris region. This mask excludes any areas where occlusions or noise are present.</p>
                <p>The final result of the segmentation process is a clean, masked iris image that can be used for feature extraction in the subsequent steps.</p>
                <pre><code class="language-python">
# Example of iris masking
def create_iris_mask(iris_image, pupil_boundary, iris_boundary, eyelid_mask, noise_mask):
    iris_mask = apply_masks(iris_image, pupil_boundary, iris_boundary, eyelid_mask, noise_mask)
    return iris_mask
</code></pre>
                <ul>
                    <li><strong>Iris mask</strong>: A binary mask that highlights the valid iris region while excluding areas affected by noise or occlusion.</li>
                </ul>
            </article>

            <article>
                <h3>4. Haar Feature Extraction in Iris Biometric Systems</h3>
                <p>Haar feature extraction is a method used in image processing for identifying patterns and structures. It was first introduced by Alfred Haar and has been widely applied in various computer vision tasks, including face and iris recognition. Haar-like features represent the intensity difference between adjacent rectangular regions in an image, which allows for detecting edges, lines, and patterns in a computationally efficient manner.</p>
                <p>In the context of iris biometric systems, Haar features are used to extract specific texture characteristics of the iris that can then be used for matching and recognition.</p>
            </article>

            <article>
                <h4>4.1 Overview of Haar Features</h4>
                <p>Haar features are based on the difference in pixel intensities between rectangular regions. These features are simple to compute and can efficiently describe the texture and structure of an image. The Haar-like features are commonly used in the form of edge features, line features, and center-surround features, which detect contrast between adjacent areas.</p>
                <p>Each feature is computed as the weighted sum of pixels in white and black rectangular regions. The difference between these two regions is what defines the feature.</p>
                <ul>
                    <li><strong>Edge feature</strong>: Compares the intensity difference between two adjacent regions, often horizontally or vertically.</li>
                    <li><strong>Line feature</strong>: Captures intensity differences between three adjacent regions, where the middle region has a different intensity than the surrounding regions.</li>
                    <li><strong>Center-surround feature</strong>: Captures intensity differences between a central region and its surrounding area.</li>
                </ul>
            </article>

            <article>
                <h5>4.1.1 Types of Haar-like Features</h5>
                <p>Haar-like features come in different types, and they can be applied to detect different characteristics of the iris image:</p>
                <ul>
                    <li><strong>Two-rectangle feature</strong>: Compares the sum of pixel values in two adjacent rectangular regions.</li>
                    <li><strong>Three-rectangle feature</strong>: Compares three regions, where the central region is compared to the two surrounding ones.</li>
                    <li><strong>Four-rectangle feature</strong>: Compares four regions arranged in a grid-like pattern to detect patterns like textures or spots.</li>
                </ul>
                <pre><code class="language-python">
# Example: Calculation of a two-rectangle Haar feature
def haar_two_rectangle(image, region1, region2):
    return sum(region2) - sum(region1)
</code></pre>
            </article>

            <article>
                <h4>4.2 Haar Feature Extraction Process</h4>
                <p>The process of extracting Haar features from an iris image involves multiple steps, from converting the image to grayscale, to applying Haar-like feature masks, and finally creating a feature vector that will be used for classification or matching. The extraction process is fast due to the integral image method, which speeds up the computation of rectangular region sums.</p>
            </article>

            <article>
                <h5>4.2.1 Step 1: Grayscale Conversion</h5>
                <p>The first step in Haar feature extraction is converting the iris image into grayscale. This simplifies the image by reducing it to one channel, which makes the processing faster and more efficient. Haar features are computed based on intensity differences, so color information is not necessary.</p>
                <pre><code class="language-python">
// Convert image to grayscale
def convert_to_grayscale(image):
    return image.convert('L')
</code></pre>
            </article>

            <article>
                <h5>4.2.2 Step 2: Integral Image Creation</h5>
                <p>After converting the image to grayscale, the next step is to create an integral image (also known as summed area table). The integral image allows for the fast calculation of rectangular sums by storing the cumulative sum of pixel intensities from the top-left corner to any point in the image.</p>
                <p>Using the integral image, the sum of any rectangular region can be computed in constant time, making Haar feature extraction much more efficient.</p>
                <pre><code class="language-python">
// Calculate integral image for fast rectangular sum calculation
def calculate_integral_image(image):
    height, width = image.shape
    integral_img = np.zeros((height+1, width+1), dtype=np.int32)
    for i in range(1, height+1):
        for j in range(1, width+1):
            integral_img[i, j] = image[i-1, j-1] + integral_img[i-1, j] + integral_img[i, j-1] - integral_img[i-1, j-1]
    return integral_img
</code></pre>
                <ul>
                    <li><strong>Integral image</strong>: A matrix that stores cumulative pixel sums for efficient rectangular region calculations.</li>
                </ul>
            </article>

            <article>
                <h5>4.2.3 Step 3: Haar Feature Calculation</h5>
                <p>Once the integral image is computed, Haar-like features are calculated by applying feature masks (rectangular patterns) to the image. The feature masks are applied at different scales and locations within the image to capture the structural and textural differences in the iris.</p>
                <p>The difference in pixel sums between the black and white regions of each mask gives the Haar feature value.</p>
                <pre><code class="language-python">
// Calculate Haar-like features using the integral image
def calculate_haar_features(integral_img, feature_type):
    if feature_type == 'two_rectangle':
        return haar_two_rectangle(integral_img, region1, region2)
    elif feature_type == 'three_rectangle':
        return haar_three_rectangle(integral_img, region1, region2, region3)
    elif feature_type == 'four_rectangle':
        return haar_four_rectangle(integral_img, region1, region2, region3, region4)
</code></pre>
                <ul>
                    <li><strong>Feature masks</strong>: Predefined rectangular patterns applied to the image to extract texture features.</li>
                    <li><strong>Multi-scale analysis</strong>: Haar features are calculated at multiple scales to capture features of different sizes within the iris.</li>
                </ul>
            </article>

            <article>
                <h5>4.2.4 Step 4: Feature Vector Construction</h5>
                <p>After calculating the Haar features for multiple regions and scales in the image, a feature vector is constructed. This feature vector represents the unique textural information of the iris and will be used for classification or matching in the recognition system.</p>
                <p>Each element in the feature vector corresponds to a Haar feature value, and the vector typically contains several hundred or thousands of elements, depending on the resolution of the iris image and the number of Haar masks used.</p>
                <pre><code class="language-python">
// Construct the feature vector
def construct_feature_vector(haar_features):
    feature_vector = np.concatenate(haar_features)
    return feature_vector
</code></pre>
                <ul>
                    <li><strong>Feature vector</strong>: A numerical representation of the iris texture based on Haar features.</li>
                </ul>
            </article>

            <article>
                <h5>4.2.5 Step 5: Classification or Matching</h5>
                <p>Once the feature vector is constructed, it can be used for iris recognition. This can be done by either comparing the extracted features to those stored in a database (matching) or by using machine learning algorithms for classification. Haar features provide a compact and efficient way to represent the unique characteristics of an iris.</p>
                <ul>
                    <li><strong>Matching</strong>: Feature vectors are compared using similarity measures such as Euclidean distance or Hamming distance.</li>
                    <li><strong>Classification</strong>: Machine learning classifiers like SVM (Support Vector Machines) can be used to categorize feature vectors into different classes (identities).</li>
                </ul>
                <pre><code class="language-python">
# Example: Matching feature vectors
def match_feature_vectors(vector1, vector2):
    return np.linalg.norm(vector1 - vector2)
</code></pre>
            </article>

            <article>
                <h3>5. Nearest Neighbour Matching in Iris Biometric Systems</h3>
                <p>Nearest Neighbour (NN) Matching is a simple and effective technique for classification and recognition in iris biometric systems. The idea behind NN matching is to find the most similar instance (or 'neighbour') in a stored database to a given input feature vector by calculating a similarity or distance metric between them. In iris recognition, this method is used to compare the extracted feature vector from an iris image with the vectors stored in the system's database.</p>
                <p>Nearest Neighbour Matching is particularly useful for classification tasks because it relies on the principle that similar patterns (such as iris textures) are likely to belong to the same class (i.e., the same person).</p>
            </article>

            <article>
                <h4>5.1 Nearest Neighbour Matching Process</h4>
                <p>The nearest neighbour process involves several key steps that begin after extracting the feature vector from an iris image. The main goal is to find the closest match in a database of feature vectors. Below are the detailed steps of the NN matching process.</p>
            </article>

            <article>
                <h5>5.1.1 Step 1: Feature Vector Representation</h5>
                <p>After extracting features from an iris image (e.g., using Haar features or other methods), they are represented as a feature vector. This vector contains numerical values that uniquely describe the iris pattern. Each vector in the database represents the iris of a known individual.</p>
                <p>For instance, if $X$ is a feature vector from the input image, the database contains feature vectors $V_1, V_2, V_3, ..., V_n$, where $n$ is the number of stored iris templates.</p>
                <pre><code class="language-python">
# Example of feature vector for an iris image
input_feature_vector = np.array([0.25, 0.18, 0.93, ...])
database_vectors = [np.array([0.23, 0.19, 0.90, ...]), np.array([0.26, 0.17, 0.91, ...])]
</code></pre>
            </article>

            <article>
                <h5>5.1.2 Step 2: Distance Calculation</h5>
                <p>The next step is to compute the distance (or similarity) between the input feature vector and each vector in the database. The most commonly used distance metrics for NN matching in iris recognition are Euclidean distance and Hamming distance. These distances measure how 'close' or 'similar' the input vector is to the stored vectors.</p>

                <h5>5.1.2.1 Euclidean Distance</h5>
                <p>The Euclidean distance between two vectors $X$ and $Y$ is given by:</p>
                <p>$$ d(X, Y) = \sqrt{\sum_{i=1}^{n} (X_i - Y_i)^2} $$</p>
                <p>It computes the straight-line distance between the points in the vector space.</p>
                <pre><code class="language-python">
# Example: Calculate Euclidean distance
def euclidean_distance(vector1, vector2):
    return np.linalg.norm(vector1 - vector2)
</code></pre>

                <h5>5.1.2.2 Hamming Distance</h5>
                <p>The Hamming distance is often used when the feature vectors are binary (e.g., iris codes). It calculates the number of positions at which the corresponding bits differ between two binary vectors. For two binary vectors $X$ and $Y$, the Hamming distance is given by:</p>
                <p>$$ d(X, Y) = \sum_{i=1}^{n} |X_i - Y_i| $$</p>
                <pre><code class="language-python">
# Example: Calculate Hamming distance for binary vectors
def hamming_distance(vector1, vector2):
    return np.sum(np.bitwise_xor(vector1, vector2))
</code></pre>
            </article>

            <article>
                <h5>5.1.3 Step 3: Finding the Nearest Neighbour</h5>
                <p>Once the distances between the input feature vector and all vectors in the database are calculated, the nearest neighbour is the one with the smallest distance. The system will then select this nearest neighbour as the most likely match.</p>
                <p>Let $V_i$ be the feature vector in the database with the smallest distance to the input vector $X$. This is represented as:</p>
                <p>$$ \text{NN}(X) = \arg \min_{i} d(X, V_i) $$</p>
                <pre><code class="language-python">
# Example: Find the nearest neighbour based on Euclidean distance
def nearest_neighbour(input_vector, database_vectors):
    distances = [euclidean_distance(input_vector, db_vector) for db_vector in database_vectors]
    nearest_index = np.argmin(distances)
    return database_vectors[nearest_index]
</code></pre>
                <ul>
                    <li><strong>Nearest neighbour selection</strong>: The system selects the feature vector in the database with the smallest distance to the input vector.</li>
                    <li><strong>Distance metric</strong>: Either Euclidean or Hamming distance can be used, depending on the representation of the feature vectors.</li>
                </ul>
            </article>

            <article>
                <h5>5.1.4 Step 4: Classification or Recognition</h5>
                <p>After identifying the nearest neighbour, the system can perform classification or recognition. If the nearest neighbour belongs to a specific individual, the system will classify the input image as that individual. If a matching threshold is used, the system may require the distance to be below a certain value to confirm a match.</p>
                <p>If the distance is too large, the system may reject the input image as unrecognized. The threshold depends on the application and the sensitivity required for the recognition task.</p>
                <ul>
                    <li><strong>Threshold</strong>: A distance threshold can be defined to decide whether a match is valid.</li>
                    <li><strong>Rejection criteria</strong>: If no neighbour falls within the threshold, the input image is rejected as an unrecognized iris.</li>
                </ul>
                <pre><code class="language-python">
# Example: Perform classification based on nearest neighbour
def classify_iris(input_vector, database_vectors, threshold=0.5):
    nearest_vector = nearest_neighbour(input_vector, database_vectors)
    distance = euclidean_distance(input_vector, nearest_vector)
    if distance < threshold:
        return "Match found"
    else:
        return "No match"
</code></pre>
            </article>

            <article>
                <h4>5.2 Advantages and Limitations of Nearest Neighbour Matching</h4>
                <p>Nearest Neighbour Matching is widely used due to its simplicity and effectiveness in many biometric systems, including iris recognition. However, it also comes with certain trade-offs.</p>
                <ul>
                    <li><strong>Advantages</strong>:
                        <ul>
                            <li>Simple and easy to implement.</li>
                            <li>Effective for small datasets where the search space is not too large.</li>
                            <li>No need for training a model – matching is based purely on the distance metric.</li>
                        </ul>
                    </li>
                    <li><strong>Limitations</strong>:
                        <ul>
                            <li>Computationally expensive for large datasets, as distances must be calculated for every stored vector.</li>
                            <li>Prone to noise and outliers in feature vectors.</li>
                            <li>May struggle with scalability in large-scale systems.</li>
                        </ul>
                    </li>
                </ul>
            </article>

            <article>
                <h3>6. Template Storage and Security</h3>
                <p>Once the feature vector (or iris template) has been extracted from the iris image, it must be securely stored in the system's database for future matching and recognition. Ensuring the security and privacy of these templates is critical, as compromised biometric data cannot be easily revoked or changed like passwords. This section discusses the storage mechanisms and security measures involved in iris biometric systems.</p>
            </article>

            <article>
                <h4>6.1 Iris Template Creation</h4>
                <p>Before storage, the extracted features are transformed into a compact representation known as the iris template. This template is a binary or numeric vector that encodes the unique characteristics of an individual's iris. The size of the template is typically small (a few hundred to a thousand bytes) to allow for efficient storage and fast matching.</p>
                <p>The template creation process ensures that the data is both compact and robust to small variations, such as changes in lighting or pupil dilation.</p>
                <pre><code class="language-python">
# Example of iris template creation
def create_iris_template(feature_vector):
    return np.packbits(np.array(feature_vector > 0.5, dtype=int))
</code></pre>
                <ul>
                    <li><strong>Compactness</strong>: The template is highly compressed to reduce storage requirements.</li>
                    <li><strong>Robustness</strong>: The template is designed to tolerate minor variations in the iris image while still producing consistent matching results.</li>
                </ul>
            </article>

            <article>
                <h4>6.2 Secure Storage Mechanisms</h4>
                <p>Once the iris template is created, it is stored in the system's database. To protect the template from unauthorized access, several security measures are employed, such as encryption, hashing, and access control mechanisms.</p>
            </article>

            <article>
                <h5>6.2.1 Template Encryption</h5>
                <p>To ensure that the stored iris templates cannot be easily accessed or tampered with, encryption techniques are used. The templates are encrypted before being stored in the database, and decryption is only performed when matching is required. Common encryption algorithms such as Advanced Encryption Standard (AES) are widely used to provide strong security.</p>
                <p>Encryption protects the biometric data even if an attacker gains access to the storage database.</p>
                <pre><code class="language-python">
# Example of encrypting an iris template using AES
from Crypto.Cipher import AES
from Crypto.Random import get_random_bytes

def encrypt_template(template, key):
    cipher = AES.new(key, AES.MODE_EAX)
    ciphertext, tag = cipher.encrypt_and_digest(template)
    return cipher.nonce, ciphertext, tag
</code></pre>
                <ul>
                    <li><strong>Data confidentiality</strong>: Encryption ensures that the template remains unreadable to unauthorized users.</li>
                    <li><strong>Security</strong>: Strong encryption algorithms like AES-256 provide robust protection against attacks.</li>
                </ul>
            </article>

            <article>
                <h5>6.2.2 Hashing for Template Protection</h5>
                <p>In some systems, templates are hashed before storage. Hashing is a one-way function that converts the template into a fixed-size string, making it impossible to reverse-engineer the original template from the hash. When a new iris is scanned, its template is also hashed, and the hashed templates are compared for matching.</p>
                <p>A cryptographic hash function like SHA-256 is often used for this purpose.</p>
                <pre><code class="language-python">
# Example of hashing an iris template using SHA-256
import hashlib

def hash_template(template):
    return hashlib.sha256(template).hexdigest()
</code></pre>
                <ul>
                    <li><strong>Irreversibility</strong>: The original template cannot be reconstructed from the hash.</li>
                    <li><strong>Integrity</strong>: Hashing ensures that the stored data has not been altered.</li>
                </ul>
            </article>

            <article>
                <h5>6.2.3 Template Matching with Secure Multi-Party Computation (SMPC)</h5>
                <p>In privacy-preserving systems, Secure Multi-Party Computation (SMPC) allows for matching iris templates without revealing the actual templates to either party. This is useful in cloud-based systems where biometric data may be processed off-site. SMPC protocols enable the system to perform template matching securely, ensuring that neither the server nor the client can access each other’s raw biometric data.</p>
                <pre><code class="language-python">
# Example of SMPC-based secure matching (simplified)
def secure_template_matching(encrypted_template1, encrypted_template2):
    # Perform secure comparison of encrypted templates
    result = smpc_compare(encrypted_template1, encrypted_template2)
    return result
</code></pre>
                <ul>
                    <li><strong>Privacy</strong>: Both parties perform matching without revealing their raw templates to each other.</li>
                    <li><strong>Security</strong>: Data is protected during both storage and computation.</li>
                </ul>
            </article>

            <article>
                <h4>6.3 Access Control Mechanisms</h4>
                <p>Access control mechanisms are used to ensure that only authorized personnel or systems can access the stored templates. This is critical in multi-user systems where the database might store biometric templates for a large number of individuals.</p>
                <ul>
                    <li><strong>Role-based access control (RBAC)</strong>: Defines roles with specific access permissions to the database.</li>
                    <li><strong>Multi-factor authentication (MFA)</strong>: Adds an extra layer of security, requiring additional credentials (e.g., passwords, tokens) to access biometric data.</li>
                </ul>
            </article>

            <article>
                <h5>6.3.1 Biometric Encryption</h5>
                <p>Biometric encryption refers to the process where the biometric data itself (i.e., the iris template) is used as part of the cryptographic key generation process. This adds an additional layer of security by tightly integrating the biometric data into the encryption framework.</p>
                <pre><code class="language-python">
# Example of generating an encryption key using biometric data
def generate_key_from_biometrics(iris_template):
    return hashlib.sha256(iris_template).digest()
</code></pre>
                <ul>
                    <li><strong>Integration</strong>: Combines the biometric data with the cryptographic process, enhancing security.</li>
                    <li><strong>Privacy</strong>: Ensures that the biometric data is never stored in plain form.</li>
                </ul>
            </article>

            <article>
                <h4>6.4 Privacy Considerations</h4>
                <p>In addition to security, privacy is a key concern in biometric systems. The storage and use of biometric data, including iris templates, must comply with relevant privacy regulations (e.g., GDPR). Proper anonymization techniques can be applied to ensure that biometric data is not linked to identifiable personal information unless necessary for the recognition process.</p>
                <ul>
                    <li><strong>Anonymization</strong>: Ensures that stored templates are not directly linked to personal identifiers unless absolutely necessary.</li>
                    <li><strong>Compliance</strong>: Systems must follow local and international privacy regulations for biometric data.</li>
                </ul>
            </article>

            <article>
                <h5>6.4.1 Template Deletion Policies</h5>
                <p>Systems must implement clear policies for template deletion, especially when individuals opt out of biometric systems or when templates are no longer required. Secure deletion ensures that templates are completely removed from the system and cannot be recovered later.</p>
                <pre><code class="language-python">
# Example of secure deletion of an iris template
def secure_delete_template(template_id, database):
    # Overwrite and remove the template from the database
    database.remove(template_id)
</code></pre>
                <ul>
                    <li><strong>Secure deletion</strong>: Ensures that biometric templates are completely removed from the system when no longer needed.</li>
                    <li><strong>Compliance</strong>: Meets regulatory requirements for data retention and deletion.</li>
                </ul>
            </article>

            <article>
                <h3>7. Algorithms Used in Iris Biometric Systems</h3>
                <p>Iris biometric systems use a combination of image processing, feature extraction, and matching algorithms to accurately identify or verify an individual based on their iris patterns. The algorithms are designed to handle the specific challenges of iris recognition, such as noise, occlusions, and variations in lighting or distance. This section will describe the algorithms used in detail, including segmentation, feature extraction, and matching techniques.</p>
            </article>

            <article>
                <h4>7.1 Segmentation Algorithms</h4>
                <p>Segmentation is the first and crucial step in iris recognition, where the iris region is isolated from the rest of the eye image. The accuracy of segmentation directly affects the performance of subsequent stages, as poor segmentation can lead to incorrect feature extraction and matching.</p>
            </article>

            <article>
                <h5>7.1.1 Daugman’s Integro-Differential Operator</h5>
                <p>This algorithm is one of the most commonly used methods for detecting the boundaries of the iris and pupil. It searches for circular boundaries in the iris and pupil by maximizing the changes in pixel intensity along circular contours.</p>
                <p>The operator is defined as:</p>
                <p>$$ \max_{r, x_0, y_0} \left| \frac{\partial}{\partial r} \int_r \int_0^{2\pi} I(x_0 + r\cos\theta, y_0 + r\sin\theta) \, d\theta \, dr \right| $$</p>
                <p>Where:</p>
                <ul>
                    <li><strong>$r$</strong>: Radius of the circle (for both iris and pupil boundaries).</li>
                    <li><strong>$x_0, y_0$</strong>: Coordinates of the center of the circle.</li>
                    <li><strong>$I(x, y)$</strong>: Intensity of the image at point $(x, y)$.</li>
                </ul>
                <p>The operator works by iteratively testing different circles (with varying radii and positions) and selecting the circle where the gradient is maximized.</p>
                <pre><code class="language-python">
# Example of Daugman's Integro-Differential Operator for iris segmentation
def daugman_operator(image, radii_range, center_x_range, center_y_range):
    best_circle = None
    max_gradient = -float('inf')
    
    for r in radii_range:
        for x0 in center_x_range:
            for y0 in center_y_range:
                gradient = calculate_gradient(image, r, x0, y0)
                if gradient > max_gradient:
                    max_gradient = gradient
                    best_circle = (x0, y0, r)
    
    return best_circle
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used to detect the outer boundary of the iris and the inner boundary of the pupil.</li>
                    <li><strong>Strength</strong>: Robust to noise and occlusions.</li>
                    <li><strong>Limitation</strong>: Computationally expensive, especially for high-resolution images.</li>
                </ul>
            </article>

            <article>
                <h5>7.1.2 Circular Hough Transform</h5>
                <p>The Circular Hough Transform is another popular method for detecting circular shapes in an image, such as the boundaries of the pupil and iris. It transforms points in the image into a parameter space (radius and center coordinates) and searches for points that fit a circle.</p>
                <p>The parametric equation for a circle is:</p>
                <p>$$ (x - x_0)^2 + (y - y_0)^2 = r^2 $$</p>
                <p>The algorithm finds the maximum in the accumulator space, which corresponds to the best-fitting circle in the image.</p>
                <pre><code class="language-python">
# Example of Circular Hough Transform for iris boundary detection
def hough_circular_transform(image, min_radius, max_radius):
    edges = detect_edges(image)
    accumulator = np.zeros((image.shape[0], image.shape[1], max_radius - min_radius))
    
    for x in range(image.shape[0]):
        for y in range(image.shape[1]):
            if edges[x, y]:
                for r in range(min_radius, max_radius):
                    for theta in range(0, 360):
                        a = int(x - r * np.cos(theta * np.pi / 180))
                        b = int(y - r * np.sin(theta * np.pi / 180))
                        if 0 <= a < image.shape[0] and 0 <= b < image.shape[1]:
                            accumulator[a, b, r - min_radius] += 1
                            
    max_value = np.max(accumulator)
    best_circle = np.unravel_index(np.argmax(accumulator), accumulator.shape)
    return best_circle
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used to detect both the pupil and iris boundaries.</li>
                    <li><strong>Strength</strong>: Effective for detecting circular shapes with noise or occlusion.</li>
                    <li><strong>Limitation</strong>: Sensitive to image resolution and can be slow for large images.</li>
                </ul>
            </article>

            <article>
                <h4>7.2 Feature Extraction Algorithms</h4>
                <p>Once the iris is segmented, feature extraction is performed to capture the unique patterns of the iris. These patterns are converted into a feature vector or code, which can be used for matching in the recognition process.</p>
            </article>

            <article>
                <h5>7.2.1 Gabor Wavelet Transform</h5>
                <p>Gabor wavelet is a commonly used feature extraction technique for iris recognition, as it effectively captures texture information. A Gabor filter is a linear filter that analyzes the image in both spatial and frequency domains, providing information on the local texture of the iris.</p>
                <p>The 2D Gabor wavelet is given by:</p>
                <p>$$ G(x, y; \lambda, \theta, \sigma, \gamma) = \exp \left( -\frac{x'^2 + \gamma^2 y'^2}{2\sigma^2} \right) \cdot \exp \left( i \left( 2\pi \frac{x'}{\lambda} + \psi \right) \right) $$</p>
                <p>Where:</p>
                <ul>
                    <li><strong>$x', y'$</strong>: Coordinates after rotation by angle $\theta$.</li>
                    <li><strong>$\lambda$</strong>: Wavelength of the sinusoidal factor.</li>
                    <li><strong>$\sigma$</strong>: Standard deviation of the Gaussian envelope.</li>
                    <li><strong>$\gamma$</strong>: Spatial aspect ratio (controls the ellipticity of the Gaussian).</li>
                </ul>
                <pre><code class="language-python">
# Example of Gabor filter for feature extraction
def gabor_filter(image, wavelength, orientation):
    kernel = cv2.getGaborKernel((21, 21), 4.0, orientation, wavelength, 0.5, 0, ktype=cv2.CV_32F)
    filtered_image = cv2.filter2D(image, cv2.CV_8UC3, kernel)
    return filtered_image
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used to extract the texture and frequency information of the iris.</li>
                    <li><strong>Strength</strong>: Captures both spatial and frequency information, making it robust to variations in lighting and noise.</li>
                    <li><strong>Limitation</strong>: Computationally intensive, especially when applied at multiple scales and orientations.</li>
                </ul>
            </article>

            <article>
                <h5>7.2.2 Haar-like Feature Extraction</h5>
                <p>Haar-like feature extraction is another method used for iris recognition, as described earlier in Section 4. It is based on the intensity differences between adjacent rectangular regions in an image. This technique is computationally efficient and useful for detecting edges and lines in the iris.</p>
                <p>The features are calculated as the difference in pixel sums between black and white regions using the integral image technique for fast computation.</p>
                <pre><code class="language-python">
# Example of Haar-like feature extraction using integral image
def haar_feature(image):
    integral_image = calculate_integral_image(image)
    feature_vector = calculate_haar_features(integral_image, 'two_rectangle')
    return feature_vector
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used to capture structural differences in the iris image.</li>
                    <li><strong>Strength</strong>: Fast and computationally efficient.</li>
                    <li><strong>Limitation</strong>: Sensitive to noise and may not capture subtle texture patterns.</li>
                </ul>
            </article>

            <article>
                <h5>7.2.3 1D Log-Gabor Filter</h5>
                <p>The 1D Log-Gabor filter is another popular technique for iris feature extraction. Unlike Gabor filters, Log-Gabor filters do not have DC components, making them better suited for representing high-frequency information. The logarithmic distribution of the frequency response allows the filter to capture finer details in the iris.</p>
                <p>The transfer function of the 1D Log-Gabor filter is given by:</p>
                <p>$$ G(f) = \exp \left( - \frac{\log(f/f_0)^2}{2(\log(\sigma/f_0))^2} \right) $$</p>
                <p>Where $f_0$ is the center frequency, and $\sigma$ controls the bandwidth.</p>
                <pre><code class="language-python">
# Example of 1D Log-Gabor filter
def log_gabor_filter(iris_image, center_frequency, bandwidth):
    rows, cols = iris_image.shape
    log_gabor = np.zeros((rows, cols))
    for i in range(rows):
        for j in range(cols):
            log_gabor[i, j] = np.exp(-(np.log(j/center_frequency)**2) / (2*(np.log(bandwidth/center_frequency))**2))
    
    filtered_image = np.fft.ifft2(np.fft.fft2(iris_image) * log_gabor)
    return np.abs(filtered_image)
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used for extracting detailed patterns from the iris, particularly useful for capturing fine texture details.</li>
                    <li><strong>Strength</strong>: Excellent for high-frequency detail extraction and has no DC component.</li>
                    <li><strong>Limitation</strong>: Computationally complex.</li>
                </ul>
            </article>

            <article>
                <h4>7.3 Matching Algorithms</h4>
                <p>After feature extraction, the final step in the iris recognition system is matching the extracted feature vector with those stored in the database to determine the identity of the individual.</p>
            </article>

            <article>
                <h5>7.3.1 Hamming Distance Matching</h5>
                <p>Hamming distance is commonly used for matching binary feature vectors (such as iris codes). The Hamming distance between two binary strings is the number of positions where the corresponding bits are different. The formula for Hamming distance is:</p>
                <p>$$ d(X, Y) = \sum_{i=1}^{n} |X_i - Y_i| $$</p>
                <p>Where $X$ and $Y$ are the two binary feature vectors, and $n$ is their length. The result is a number between 0 and 1, where 0 means identical vectors, and 1 means completely different vectors.</p>
                <pre><code class="language-python">
# Example of Hamming distance matching
def hamming_distance(vector1, vector2):
    return np.sum(np.bitwise_xor(vector1, vector2)) / len(vector1)
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used to match binary feature vectors (iris codes) in the database.</li>
                    <li><strong>Strength</strong>: Fast and computationally simple.</li>
                    <li><strong>Limitation</strong>: Works only with binary vectors; performance degrades with noisy or corrupted data.</li>
                </ul>
            </article>

            <article>
                <h5>7.3.2 Euclidean Distance Matching</h5>
                <p>Euclidean distance is often used for matching non-binary feature vectors. It calculates the straight-line distance between two points in a multidimensional space. The formula for Euclidean distance between vectors $X$ and $Y$ is:</p>
                <p>$$ d(X, Y) = \sqrt{\sum_{i=1}^{n} (X_i - Y_i)^2} $$</p>
                <p>This method is often used for feature vectors generated by techniques like Gabor filters or Log-Gabor filters, where the feature vectors are real-valued rather than binary.</p>
                <pre><code class="language-python">
# Example of Euclidean distance matching
def euclidean_distance(vector1, vector2):
    return np.linalg.norm(vector1 - vector2)
</code></pre>
                <ul>
                    <li><strong>Application</strong>: Used to match non-binary feature vectors in the database.</li>
                    <li><strong>Strength</strong>: Works well with real-valued vectors.</li>
                    <li><strong>Limitation</strong>: More computationally intensive than Hamming distance.</li>
                </ul>
            </article>

            <article>
                <h5>7.3.3 Nearest Neighbour Matching</h5>
                <p>As explained in Section 5, Nearest Neighbour matching is a method used to find the closest match between the input feature vector and the stored feature vectors in the database. It calculates the distance between the input vector and all database vectors, selecting the one with the smallest distance as the match.</p>
                <ul>
                    <li><strong>Application</strong>: Used for both binary and non-binary vectors depending on the distance metric used (Hamming or Euclidean).</li>
                    <li><strong>Strength</strong>: Simple and effective for small to medium datasets.</li>
                    <li><strong>Limitation</strong>: Slow for large datasets, and computationally expensive.</li>
                </ul>
            </article>

            <article>
                <h3>8. Results of Iris Biometric Systems</h3>
                <p>The results of an iris biometric system are derived from the final stage of the matching process, where the system attempts to identify or verify an individual based on the comparison between the extracted feature vector from a newly captured iris image and those stored in the system's database. These results are measured using various performance metrics, including accuracy, false match rates, false non-match rates, and processing times. This section provides an overview of these result parameters and the factors affecting the system’s performance.</p>
            </article>

            <article>
                <h4>8.1 Performance Metrics</h4>
                <p>Performance metrics are essential for evaluating the accuracy and reliability of iris recognition systems. Commonly used metrics include False Match Rate (FMR), False Non-Match Rate (FNMR), and Receiver Operating Characteristic (ROC) curves.</p>
            </article>

            <article>
                <h5>8.1.1 False Match Rate (FMR)</h5>
                <p>The False Match Rate (FMR) is the probability that the system incorrectly matches the input iris feature vector to a different individual in the database. In other words, it measures how often the system falsely recognizes an unauthorized user as a match.</p>
                <p>FMR is calculated as:</p>
                <p>$$ FMR = \frac{\text{Number of False Matches}}{\text{Total Number of Impostor Attempts}} $$</p>
                <pre><code class="language-python">
# Example: Calculation of FMR
def calculate_fmr(false_matches, total_impostor_attempts):
    return false_matches / total_impostor_attempts
</code></pre>
                <ul>
                    <li><strong>Low FMR</strong>: Indicates that the system is secure, as it rarely falsely matches impostors.</li>
                    <li><strong>High FMR</strong>: Suggests that the system is prone to security breaches.</li>
                </ul>
            </article>

            <article>
                <h5>8.1.2 False Non-Match Rate (FNMR)</h5>
                <p>The False Non-Match Rate (FNMR) is the probability that the system fails to match the input iris feature vector with the correct one in the database. This occurs when the system rejects a legitimate user.</p>
                <p>FNMR is calculated as:</p>
                <p>$$ FNMR = \frac{\text{Number of False Non-Matches}}{\text{Total Number of Genuine Attempts}} $$</p>
                <pre><code class="language-python">
# Example: Calculation of FNMR
def calculate_fnmr(false_non_matches, total_genuine_attempts):
    return false_non_matches / total_genuine_attempts
</code></pre>
                <ul>
                    <li><strong>Low FNMR</strong>: Indicates that the system rarely fails to recognize legitimate users.</li>
                    <li><strong>High FNMR</strong>: Suggests that the system may reject many legitimate users, leading to inconvenience.</li>
                </ul>
            </article>

            <article>
                <h5>8.1.3 Equal Error Rate (EER)</h5>
                <p>The Equal Error Rate (EER) is a common measure of system performance where the False Match Rate (FMR) and False Non-Match Rate (FNMR) are equal. The EER is a single number that reflects the overall trade-off between security (low FMR) and usability (low FNMR). A lower EER indicates better overall system performance.</p>
                <p>The EER can be visually interpreted from the Receiver Operating Characteristic (ROC) curve, where the point on the curve corresponding to the intersection of FMR and FNMR defines the EER.</p>
                <pre><code class="language-python">
# Example: Plotting ROC curve and calculating EER
def calculate_eer(fmr_values, fnmr_values):
    return np.min(np.abs(fmr_values - fnmr_values))
</code></pre>
                <ul>
                    <li><strong>Low EER</strong>: Indicates a well-balanced system with good security and usability.</li>
                    <li><strong>High EER</strong>: Suggests that the system struggles to maintain both security and usability.</li>
                </ul>
            </article>

            <article>
                <h5>8.1.4 Receiver Operating Characteristic (ROC) Curve</h5>
                <p>The ROC curve is a graphical representation of the system’s performance, plotting the False Match Rate (FMR) against the False Non-Match Rate (FNMR) at various decision thresholds. The ideal system has a curve that quickly rises to the top-left corner (low FMR and FNMR), while a poor system has a curve that follows the diagonal line.</p>
                <pre><code class="language-python">
# Example: Generating an ROC curve
import matplotlib.pyplot as plt

def plot_roc(fmr_values, fnmr_values):
    plt.plot(fmr_values, fnmr_values, label='ROC curve')
    plt.xlabel('FMR')
    plt.ylabel('FNMR')
    plt.title('ROC Curve for Iris Recognition System')
    plt.legend()
    plt.show()
</code></pre>
                <ul>
                    <li><strong>Steep ROC curve</strong>: Indicates a highly accurate system.</li>
                    <li><strong>Flat ROC curve</strong>: Indicates a system with poor performance, where the trade-off between FMR and FNMR is not favorable.</li>
                </ul>
            </article>

            <article>
                <h4>8.2 Factors Affecting System Performance</h4>
                <p>Several factors affect the overall performance and accuracy of an iris biometric system. These include image quality, segmentation accuracy, environmental conditions, and the number of stored templates.</p>
            </article>

            <article>
                <h5>8.2.1 Image Quality</h5>
                <p>The quality of the captured iris image plays a significant role in the system’s performance. Low-quality images with occlusions, reflections, or noise can lead to inaccurate feature extraction and increased error rates. Common techniques to ensure high image quality include near-infrared (NIR) illumination and high-resolution sensors.</p>
                <ul>
                    <li><strong>High-quality images</strong>: Lead to better segmentation and feature extraction, improving recognition performance.</li>
                    <li><strong>Low-quality images</strong>: Increase FMR and FNMR, leading to poor recognition results.</li>
                </ul>
            </article>

            <article>
                <h5>8.2.2 Segmentation Accuracy</h5>
                <p>The accuracy of the segmentation process, where the iris is isolated from the surrounding pupil, sclera, and eyelids, is critical. Poor segmentation can result in incorrect feature extraction and degraded matching performance.</p>
                <ul>
                    <li><strong>Accurate segmentation</strong>: Leads to better feature extraction and matching accuracy.</li>
                    <li><strong>Inaccurate segmentation</strong>: Introduces noise and occlusions in the extracted features, leading to false matches or non-matches.</li>
                </ul>
            </article>

            <article>
                <h5>8.2.3 Environmental Conditions</h5>
                <p>Environmental factors such as lighting, camera angle, and distance between the subject and the camera can affect the quality of the captured iris image. Consistent lighting (preferably NIR) and optimal positioning reduce variations and improve recognition performance.</p>
                <ul>
                    <li><strong>Controlled environments</strong>: Ensure consistent image quality and system performance.</li>
                    <li><strong>Uncontrolled environments</strong>: Lead to increased noise and degraded performance due to varying lighting or occlusions.</li>
                </ul>
            </article>

            <article>
                <h5>8.2.4 Database Size</h5>
                <p>The size of the iris template database impacts both recognition accuracy and system performance. Larger databases require more time and computational resources to find the correct match. However, advancements in indexing and search algorithms (e.g., KD-trees or hashing) help maintain performance.</p>
                <ul>
                    <li><strong>Small databases</strong>: Lead to faster matching but may lack the necessary templates for large-scale systems.</li>
                    <li><strong>Large databases</strong>: Provide more comprehensive coverage but may suffer from slower matching without optimization.</li>
                </ul>
            </article>

            <article>
                <h4>8.3 Real-World Performance Results</h4>
                <p>In practice, iris biometric systems have been shown to achieve very high levels of accuracy. The EER for modern iris recognition systems is often reported to be below 1%, and in some cases, as low as 0.1%. However, the exact performance varies depending on the environmental conditions, quality of the images, and the size of the database.</p>
                <ul>
                    <li><strong>High accuracy</strong>: Iris recognition is one of the most accurate biometric technologies, with FMR and FNMR values typically very low.</li>
                    <li><strong>Scalability</strong>: Large-scale deployments have demonstrated that iris systems can scale effectively while maintaining high accuracy, provided that the database and search algorithms are optimized.</li>
                </ul>
            </article>

        </main>

        <script> copyright("all"); </script>

    </body>

</html>