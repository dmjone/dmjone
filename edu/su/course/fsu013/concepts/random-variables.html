<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>Random Variables: FSU013 - Shoolini U</title>
        <meta name="description" content="Learn the Random Variables for efficient working in the FSU013 course at Shoolini University. Explore the Expected Value, Variance, Standard deviation, MGF, PGF, CS with dmj.one's educational initiative.">

        <meta property="og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> -->
        <!-- <script>document.addEventListener('DOMContentLoaded', () => { renderMathInElement(document.body, { delimiters: [{ left: '$$', right: '$$', display: true }, { left: '$', right: '$', display: false }, { left: '\\\\(', right: '\\\\)', display: false }, { left: '\\\\[', right: '\\\\]', display: true }], throwOnError: false }); });</script> -->
        <!--<script>
                                document.addEventListener("DOMContentLoaded", function() {
                                    renderMathInElement(document.body, {
                delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
                ],
                throwOnError: false
                });
                });
        </script> -->
    </head>

    <body>

        <script>header_author("dm");</script>

        <main>
            <article>
                <h2 class="text-center">
                    Random Variables
                </h2>
                <div class="container mt-4 w-100 w-xl-75">
                    <div class="accordion" id="toc">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="h1">
                                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#c1" aria-controls="c1" aria-expanded="false">
                                    <i class="fas fa-book"></i> <strong>&nbsp;Table of Contents</strong>
                                </button>
                            </h2>
                            <div id="c1" class="accordion-collapse collapse" aria-labelledby="h1" data-bs-parent="#toc">
                                <div class="accordion-body">
                                    <ol class="list-unstyled p-0 m-0">
                                        <li class="p-1"><a href="#intro"><i class="fas fa-chevron-circle-right"></i> Introduction</a></li>
                                        <li class="p-1"><a href="#types"><i class="fas fa-chevron-circle-right"></i> Types of Random Variables</a></li>
                                        <li class="p-1"><a href="#probability-distributions"><i class="fas fa-chevron-circle-right"></i> Probability Distributions</a></li>
                                        <li class="p-1"><a href="#relations-among-distributions"><i class="fas fa-chevron-circle-right"></i> Relations Among Distributions</a></li>
                                        <li class="p-1"><a href="#covariance-correlation"><i class="fas fa-chevron-circle-right"></i> Covariance & Correlation</a></li>
                                    </ol>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </article>

            <article id="intro">
                <h3>1. Random Variables</h3>
                <p>In the study of probability and statistics, a random variable can be thought of as a variable whose value is subject to variations due to chance (i.e., randomness, in a mathematical sense). A random variable can take on a set of possible different values, each with an associated probability, in contrast to other mathematical variables.</p>
            </article>

            <article id="types">
                <h3>2. Types of Random Variables</h3>
                <p>There are two main types of random variables, Discrete Random Variables (DRVs) and Continuous Random Variables (CRVs). Discrete random variables take on a countable number of distinct values. Examples of DRVs include things like the number of heads when flipping three coins or the number of defective items in a batch. In contrast, continuous random variables can take on any value within a specified range, and that range can be infinite. Examples include the height of a person or the time spent waiting for a bus.</p>
            </article>

            <article id="discrete-random-variables">
                <h4>2.1 Discrete Random Variables (DRVs)</h4>
                <p>A Discrete Random Variable is one which may take on only a countable number of distinct values such as 0,1,2,3,4,... etc. Probability distribution function (PDF) for discrete random variables is given by:</p>
                <p>$P(X = x) = p(x), \quad \text{for all } x$</p>
                <p>And the cumulative distribution function (CDF) is:</p>
                <p>$F(x) = P(X \leq x) = \sum_{t \leq x} p(t), \quad \text{for all } x$</p>
                <p>The expected value (or mean, denoted $\mu$), variance (denoted $\sigma^2$), and standard deviation (denoted $\sigma$) of a discrete random variable are calculated using the following formulas:</p>
                <p>Expected value: $\mu = E(X) = \sum_{x} xP(X = x)$</p>
                <p>Variance: $\sigma^2 = Var(X) = \sum_{x} (x - \mu)^2 P(X = x)$</p>
                <p>Standard deviation: $\sigma = \sqrt{Var(X)}$</p>
            </article>

            <article id="continuous-random-variables">
                <h4>2.2 Continuous Random Variables (CRVs)</h4>
                <p>A Continuous Random Variable is one which takes an infinite number of possible values. Continuous Random Variables are usually measurements. Examples include height, weight, the amount of sugar in an orange, the time required to run a mile. Probability density function (PDF) for continuous random variables is given by:</p>
                <p>$f(x) \geq 0, \quad \text{for all } x$</p>
                <p>And the cumulative distribution function (CDF) is:</p>
                <p>$F(x) = P(X \leq x) = \int_{-\infty}^{x} f(t) dt, \quad \text{for all } x$</p>
                <p>The expected value, variance, and standard deviation of a continuous random variable are calculated using the following formulas:</p>
                <p>Expected value: $\mu = E(X) = \int_{-\infty}^{\infty} x f(x) dx$</p>
                <p>Variance: $\sigma^2 = Var(X) = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx$</p>
                <p>Standard deviation: $\sigma = \sqrt{Var(X)}$</p>
            </article>

            <article id="probability-distributions">
                <h3>3. Probability Distributions</h3>
                <p>A probability distribution describes how the values of a random variable is distributed. It is defined by the probability distribution function for discrete variables and the probability density function for continuous variables. The cumulative distribution function, which is the cumulative sum of the probabilities, applies to both discrete and continuous variables.</p>
            </article>

            <article id="binomial-distribution">
                <h4>3.1 Binomial Distribution</h4>
                <p>The binomial distribution with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$ independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome. A success/failure experiment is also called a Bernoulli experiment or Bernoulli trial. The Binomial distribution is used when there are exactly two mutually exclusive outcomes of a trial. These outcomes are appropriately labeled "success" and "failure".</p>
                <p>The probability mass function (PMF) of a binomial distribution is:</p>
                <p>$P(X=k) = C(n, k) p^k (1-p)^{n-k}, \quad k = 0, 1, 2, ..., n$</p>
                <p>where $C(n, k)$ is the binomial coefficient. The expected value, variance, and standard deviation are given by:</p>
                <p>Expected value: $\mu = np$</p>
                <p>Variance: $\sigma^2 = np(1-p)$</p>
                <p>Standard deviation: $\sigma = \sqrt{np(1-p)}$</p>
            </article>
            <article id="binomial-distribution-mgf-cf-pgf">
                <h5>3.1.1 Moment Generating Function, Characteristic Function, and Probability Generating Function for Binomial Distribution</h5>
                <p>The Moment Generating Function (MGF) of a Binomial distribution is given by:</p>
                <p>$M(t) = E[e^{tX}] = (pe^t + q)^n$</p>
                <p>The Characteristic Function (CF) of a Binomial distribution is given by:</p>
                <p>$\phi(t) = E[e^{itX}] = (pe^{it} + q)^n$</p>
                <p>The Probability Generating Function (PGF) of a Binomial distribution is given by:</p>
                <p>$G(z) = E[z^X] = (pz + q)^n$</p>
            </article>
            <article id="trivia-binomial-distribution">
                <h5>3.1.2 Trivia: Binomial Distribution</h5>
                <p>Did you know that the term "binomial distribution" is derived from the fact that each random variable represents 'two' possible outcomes (success or failure)? Also, the shape of the binomial distribution changes based on the probability of success ($p$) and the number of trials ($n$). For instance, if the probability of success is high, the distribution is skewed to the right. If the probability of success is low, it is skewed to the left. If the probability is around 0.5, the distribution resembles a symmetric bell shape, much like the normal distribution.</p>
            </article>

            <article id="poisson-distribution">
                <h4>3.2 Poisson Distribution</h4>
                <p>The Poisson distribution is the discrete probability distribution of the number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.</p>
                <p>The PMF of a Poisson distribution is:</p>
                <p>$P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, 2, ..., \infty$</p>
                <p>where $\lambda$ is the average rate of value. The expected value, variance, and standard deviation are given by:</p>
                <p>Expected value: $\mu = \lambda$</p>
                <p>Variance: $\sigma^2 = \lambda$</p>
                <p>Standard deviation: $\sigma = \sqrt{\lambda}$</p>
            </article>
            <article id="poisson-distribution-mgf-cf-pgf">
                <h5>3.2.1 Moment Generating Function, Characteristic Function, and Probability Generating Function for Poisson Distribution</h5>
                <p>The Moment Generating Function (MGF) of a Poisson distribution is given by:</p>
                <p>$M(t) = E[e^{tX}] = e^{\lambda(e^t - 1)}$</p>
                <p>The Characteristic Function (CF) of a Poisson distribution is given by:</p>
                <p>$\phi(t) = E[e^{itX}] = e^{\lambda(e^{it} - 1)}$</p>
                <p>The Probability Generating Function (PGF) of a Poisson distribution is given by:</p>
                <p>$G(z) = E[z^X] = e^{\lambda(z - 1)}$</p>
            </article>
            <article id="trivia-poisson-distribution">
                <h5>3.2.2 Trivia: Poisson Distribution</h5>
                <p>Did you know that the Poisson distribution is named after the French mathematician Siméon Denis Poisson? He introduced this distribution to describe the number of times a gambler would win a rarely won game of chance in a large number of tries. It's interesting that real-life phenomena such as telephone calls to a radio show, goals in a World Cup match, or decay of radioactive atoms, all exhibit the Poisson distribution.</p>
            </article>

            <article id="normal-distribution">
                <h4>3.3 Normal Distribution</h4>
                <p>The normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.</p>
                <p>The PDF of a normal distribution is:</p>
                <p>$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ - \frac{(x-\mu)^2}{2\sigma^2} }$</p>
                <p>The expected value, variance, and standard deviation are given by:</p>
                <p>Expected value: $\mu$</p>
                <p>Variance: $\sigma^2$</p>
                <p>Standard deviation: $\sigma$</p>
                <p>Normal distribution is extensively used in natural and social sciences. The reason is Central Limit Theorem (CLT), which states that, for a large enough sample, the distribution of the sample mean will approach normal distribution, regardless of the shape of the population distribution.</p>
            </article>
            <article id="normal-distribution-variate">
                <h5>3.3.1 Normal Variate</h5>
                <p>In the context of the normal distribution, the term 'normal variate' refers to a random variable that follows a normal distribution. If the random variable $X$ follows a normal distribution with mean $\mu$ and standard deviation $\sigma$, we denote it as $X \sim N(\mu, \sigma^2)$.</p>
            </article>
            <article id="normal-distribution-mgf-cf">
                <h5>3.3.2 Moment Generating Function and Characteristic Function for Normal Distribution</h5>
                <p>The Moment Generating Function (MGF) of a Normal distribution is given by:</p>
                <p>$M(t) = E[e^{tX}] = e^{\mu t + \frac{1}{2}\sigma^2 t^2}$</p>
                <p>The Characteristic Function (CF) of a Normal distribution is given by:</p>
                <p>$\phi(t) = E[e^{itX}] = e^{i\mu t - \frac{1}{2}\sigma^2 t^2}$</p>
                <p>Note: The normal distribution does not have a Probability Generating Function as it is defined only for discrete random variables.</p>
            </article>
            <article id="trivia-normal-distribution">
                <h5>3.3.3 Trivia: Normal Distribution</h5>
                <p>The normal distribution, also known as the Gaussian distribution, was first introduced by Abraham de Moivre in an article in 1733. However, it was fully developed by the German mathematician Carl Friedrich Gauss, hence the name 'Gaussian distribution'. This distribution is considered the most prominent probability distribution in statistics because it fits many natural phenomena like heights, blood pressure, IQ scores, etc. Plus, thanks to the Central Limit Theorem, it allows statisticians to make inferences about population means.</p>
            </article>

            <article id="uniform-distribution">
                <h4>3.4 Uniform Distribution</h4>
                <p>The uniform distribution is a type of probability distribution in which all outcomes are equally likely. A deck of cards has a uniform distribution because the likelihood of drawing a heart, club, diamond or spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.</p>
                <p>The PDF of a uniform distribution in the interval $[a, b]$ is:</p>
                <p>$f(x) = \frac{1}{b - a}, \quad a \leq x \leq b$</p>
                <p>The expected value, variance, and standard deviation are given by:</p>
                <p>Expected value: $\mu = \frac{a + b}{2}$</p>
                <p>Variance: $\sigma^2 = \frac{(b - a)^2}{12}$</p>
                <p>Standard deviation: $\sigma = \sqrt{\frac{(b - a)^2}{12}}$</p>
            </article>
            <article id="uniform-distribution-mgf-cf">
                <h5>3.4.1 Moment Generating Function and Characteristic Function for Uniform Distribution</h5>
                <p>The Moment Generating Function (MGF) of a Uniform distribution is given by:</p>
                <p>$M(t) = E[e^{tX}] = \frac{e^{tb} - e^{ta}}{t(b - a)}, \quad t \neq 0$</p>
                <p>The Characteristic Function (CF) of a Uniform distribution is given by:</p>
                <p>$\phi(t) = E[e^{itX}] = \frac{e^{itb} - e^{ita}}{it(b - a)}, \quad t \neq 0$</p>
                <p>Note: The uniform distribution does not have a Probability Generating Function as it is defined only for discrete random variables.</p>
            </article>
            <article id="trivia-uniform-distribution">
                <h5>3.4.2 Trivia: Uniform Distribution</h5>
                <p>One interesting aspect of the uniform distribution is that it is the maximum entropy probability distribution for a random variable X under no constraint other than that it is contained in the distribution's support. This means that for a given set of events, the uniform distribution represents the maximum ignorance about which event will occur, because they are all equally likely.</p>
            </article>

            <article id="exponential-distribution">
                <h4>3.5 Exponential Distribution</h4>
                <p>The exponential distribution is often concerned with the amount of time until some specific event occurs. For example, the amount of time (beginning now) until an earthquake occurs has an exponential distribution. Other examples include the length of time, in minutes, of long distance business telephone calls, and the amount of time, in months, a car battery lasts.</p>
                <p>The PDF of an exponential distribution with rate $\lambda$ is:</p>
                <p>$f(x) = \lambda e^{-\lambda x}, \quad x \geq 0$</p>
                <p>The expected value, variance, and standard deviation are given by:</p>
                <p>Expected value: $\mu = \frac{1}{\lambda}$</p>
                <p>Variance: $\sigma^2 = \frac{1}{\lambda^2}$</p>
                <p>Standard deviation: $\sigma = \frac{1}{\lambda}$</p>
            </article>
            <article id="exponential-distribution-mgf-cf">
                <h5>3.5.1 Moment Generating Function and Characteristic Function for Exponential Distribution</h5>
                <p>The Moment Generating Function (MGF) of an Exponential distribution is given by:</p>
                <p>$M(t) = E[e^{tX}] = \frac{\lambda}{\lambda - t}, \quad t &lt; \lambda$</p>
                <p>The Characteristic Function (CF) of an Exponential distribution is given by:</p>
                <p>$\phi(t) = E[e^{itX}] = \frac{\lambda}{\lambda - it}$</p>
                <p>Note: The exponential distribution does not have a Probability Generating Function as it is defined only for discrete random variables.</p>
            </article>
            <article id="trivia-exponential-distribution">
                <h5>3.5.2 Trivia: Exponential Distribution</h5>
                <p>An interesting fact about the exponential distribution is its 'memoryless' property. This means that the remaining time until an event occurs does not depend on how much time has already passed. For example, if you're waiting for a bus that arrives on average every 15 minutes, and you've already been waiting for 30 minutes, the time you still have to wait is still distributed the same as when you first arrived. This makes it a perfect model for 'survival analysis' in engineering.</p>
            </article>

            <article id="transformation-of-random-variables">
                <h3>4. Transformation of Random Variables</h3>
                <p>Often in practice, we're interested not only in a random variable $X$ but also in some function of the random variable $Y = g(X)$. Given the probability distribution of $X$, we may want to find the distribution of $Y$. This is known as the transformation of random variables. It's useful in various branches of statistics and probability, such as hypothesis testing and estimation theory.</p>
                <p>The transformation methods depend on whether the random variable is discrete or continuous and whether the transformation function is one-to-one or many-to-one. For a one-to-one function, we use the change of variables method, and for many-to-one functions, we employ the method of distribution functions.</p>
            </article>
            <article id="crv-transformation">
                <h4>4.1 Transformation of Continuous Random Variables</h4>
                <p>For a continuous random variable, let $Y = g(X)$ be a transformation of $X$. If the transformation is strictly increasing or decreasing, we can find the PDF of $Y$ by:</p>
                <p>$$f_Y(y) = f_X(g^{-1}(y))\left|\frac{dg^{-1}(y)}{dy}\right|$$</p>
                <p>where $f_X(x)$ is the PDF of $X$, $g^{-1}(y)$ is the inverse of the transformation function, and the absolute value indicates that we consider the magnitude of the derivative.</p>
            </article>
            <article id="drv-transformation">
                <h4>4.2 Transformation of Discrete Random Variables</h4>
                <p>For a discrete random variable, we can directly calculate the probability mass function (PMF) of $Y = g(X)$ by summing the probabilities for each value $x$ such that $g(x) = y$:</p>
                <p>$$P_Y(y) = \sum_{x:g(x) = y} P_X(x)$$</p>
                <p>where $P_X(x)$ is the PMF of $X$.</p>
            </article>

            <article id="relations-among-distributions">
                <h3>5. Relations Among Distributions</h3>
                <p>Many probability distributions are related to others, and understanding these relationships can often simplify analysis or provide insights into the underlying processes generating the data. Here, we'll explore some of the relationships among the binomial, Poisson, normal, uniform, and exponential distributions.</p>
            </article>
            <article id="relation-binomial-poisson">
                <h4>5.1 Relation Between Binomial and Poisson Distributions</h4>
                <p>The Poisson distribution can be used as an approximation to the binomial distribution under certain conditions, known as the Poisson limit theorem. Specifically, if a binomial distribution has a large number of trials ($n$) and a small probability of success ($p$) such that $np = \lambda$ is a moderate size, then the distribution is approximately Poisson with parameter $\lambda$.</p>
            </article>

            <article id="relation-binomial-normal">
                <h4>5.2 Relation Between Binomial and Normal Distributions</h4>
                <p>The binomial distribution tends to the normal distribution as the number of trials goes to infinity, thanks to the Central Limit Theorem (CLT). The conditions for this approximation are that $n$ is large (generally taken as $n > 30$), and $np(1-p)$ is also large. The corresponding normal distribution has mean $\mu = np$ and variance $\sigma^2 = np(1-p)$.</p>
            </article>

            <article id="relation-poisson-normal">
                <h4>5.3 Relation Between Poisson and Normal Distributions</h4>
                <p>Just like the binomial distribution, the Poisson distribution tends to the normal distribution as the parameter $\lambda$ goes to infinity. This is due to the Central Limit Theorem (CLT) and is usually reasonable for $\lambda > 30$. The corresponding normal distribution has mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$.</p>
            </article>

            <article id="relation-exponential-poisson">
                <h4>5.4 Relation Between Exponential and Poisson Distributions</h4>
                <p>The exponential distribution and the Poisson distribution are closely related. If the times between random events follow an exponential distribution with rate $\lambda$, then the total number of events in a given amount of time follows a Poisson distribution with parameter $\lambda t$. This connection is often used in queueing theory and reliability analysis of systems.</p>
            </article>

            <article id="relation-uniform-normal">
                <h4>5.5 Relation Between Uniform and Normal Distributions</h4>
                <p>If a large number of independent random variables, each with a standard uniform distribution, are added together, their normalized sum tends toward a normal distribution (Irwin–Hall distribution) due to the Central Limit Theorem (CLT). For instance, the sum of 12 independent standard uniform random variables (subtracting 6 to shift the mean to 0) approximates a standard normal distribution quite closely.</p>
            </article>

            <article id="covariance-correlation">
                <h3>6. Covariance and Correlation</h3>
                <p>When dealing with multiple random variables, it is often useful to examine how these variables interact with each other. Covariance and correlation are two measures that provide insights into the relationship between two random variables.</p>
            </article>
            <article id="covariance-definition">
                <h4>6.1 Covariance</h4>
                <p>Covariance is a measure of how much two random variables vary together. For two random variables $X$ and $Y$, the covariance is defined as:</p>
                <p>$$\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]$$</p>
                <p>where $E[X]$ is the expected value of $X$, and $E[Y]$ is the expected value of $Y$. If the covariance is positive, $X$ and $Y$ tend to be above their expected values at the same time. If the covariance is negative, one tends to be above its expected value when the other is below.</p>
            </article>
            <article id="correlation-definition">
                <h4>6.2 Correlation</h4>
                <p>Correlation, specifically the Pearson correlation coefficient, is a measure of the linear relationship between two random variables. It is the covariance of the two variables divided by the product of their standard deviations. The correlation coefficient is always between -1 and 1. For two random variables $X$ and $Y$, the correlation is defined as:</p>
                <p>$$\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}$$</p>
                <p>where $\text{Var}(X)$ is the variance of $X$, and $\text{Var}(Y)$ is the variance of $Y$.</p>
            </article>
        </main>

        <script>copyright("all");</script>
    </body>

</html>
