<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>



        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script> -->

        <!-- <style>
            main ul {
                list-style-type: none;
                padding: 0;
                margin: 0;
            }

            main ul li {
                margin: 0;
                padding: 0;
            }
        </style> -->




    </head>

    <body>

        <script> header_author("dm"); </script>

        <main>
            <article class="agen-tableofcontents">
                <h2 class="text-center">
                    Phases of a Compiler: Lexical Analysis
                </h2>
                <div class="d-none contentdate">2025, February 4</div>
            </article>

            <article>
                <h3>Lexical Analysis (Scanner)</h3>
                <p>Lexical Analysis is the first phase of a compiler that processes source code to convert it into meaningful tokens. It reads the program character by character, grouping them into lexemes (words) and assigning them token classifications.</p>
                <ul>
                    <li><strong>Why is it important?</strong> It simplifies parsing by abstracting raw character streams into structured tokens.</li>
                    <li><strong>Where is it used?</strong> In compilers, interpreters, and text-processing tools.</li>
                    <li><strong>When is it used?</strong> Before syntax analysis, ensuring a clean, structured input for the parser.</li>
                </ul>
            </article>


            <article>
                <h3>Tokenization</h3>
                <p>Tokenization is the process of breaking down a source program into a sequence of meaningful tokens. Each token is a fundamental unit of the language, representing a logical component like a keyword, identifier, or operator.</p>

                <h4>Token Structure</h4>
                <p>Each token consists of:</p>
                <ul>
                    <li><strong>Token Name</strong>: A unique identifier for the token category (e.g., "IDENTIFIER", "NUMBER", "KEYWORD").</li>
                    <li><strong>Lexeme</strong>: The actual sequence of characters in the source code (e.g., <code>int</code>, <code>varName</code>, <code>123</code>).</li>
                    <li><strong>Attribute</strong>: Additional information about the token (e.g., pointer to a symbol table entry).</li>
                </ul>

                <h4>Types of Tokens</h4>
                <p>Tokens are classified into different categories:</p>

                <h4>Keywords</h4>
                <p>Keywords are reserved words in a programming language that have special meanings and cannot be used as identifiers.</p>
                <ul>
                    <li>Examples in C: <code>if</code>, <code>while</code>, <code>return</code>, <code>for</code>, <code>break</code>.</li>
                    <li>Examples in Python: <code>def</code>, <code>class</code>, <code>lambda</code>, <code>yield</code>.</li>
                    <li>Examples in JavaScript: <code>function</code>, <code>var</code>, <code>let</code>, <code>const</code>.</li>
                </ul>

                <h4>Identifiers</h4>
                <p>Identifiers are user-defined names for variables, functions, and objects.</p>
                <ul>
                    <li>They must follow naming rules (e.g., in C, identifiers cannot start with a digit).</li>
                    <li>Examples: <code>totalSum</code>, <code>myFunction</code>, <code>studentAge</code>.</li>
                </ul>

                <h4>Literals</h4>
                <p>Literals are fixed values assigned to variables, such as numbers, characters, and strings.</p>
                <ul>
                    <li><strong>Integer literals</strong>: <code>42</code>, <code>-5</code>, <code>1000</code>.</li>
                    <li><strong>Floating-point literals</strong>: <code>3.14</code>, <code>0.001</code>, <code>-2.718</code>.</li>
                    <li><strong>Character literals</strong>: <code>'A'</code>, <code>'z'</code>, <code>'5'</code>.</li>
                    <li><strong>String literals</strong>: <code>"hello"</code>, <code>"CompilerDesign"</code>.</li>
                </ul>

                <h4>Operators</h4>
                <p>Operators perform computations and comparisons. They are categorized as:</p>
                <ul>
                    <li><strong>Arithmetic operators</strong>: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>.</li>
                    <li><strong>Comparison operators</strong>: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>.</li>
                    <li><strong>Logical operators</strong>: <code>&&</code> (AND), <code>||</code> (OR), <code>!</code> (NOT).</li>
                    <li><strong>Bitwise operators</strong>: <code>&</code>, <code>|</code>, <code>^</code>, <code>~</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code>.</li>
                </ul>

                <h4>Special Symbols</h4>
                <p>Special symbols are punctuation marks or delimiters that structure the code.</p>
                <ul>
                    <li><strong>Separators</strong>: <code>;</code> (end of statement), <code>,</code> (comma separator).</li>
                    <li><strong>Brackets</strong>: <code>{}</code> (block grouping), <code>[]</code> (arrays), <code>()</code> (function calls).</li>
                </ul>

                <h4>Tokenization Process</h4>
                <p>The lexical analyzer (scanner) reads the source program character by character and groups them into tokens using:</p>
                <ul>
                    <li><strong>Pattern Matching</strong>: Uses regular expressions to define token rules.</li>
                    <li><strong>Finite Automata</strong>: Determines valid sequences of characters.</li>
                    <li><strong>Buffering Techniques</strong>: Improves efficiency by reading input in chunks.</li>
                </ul>

                <h4>Example Tokenization</h4>
                <p>For the input code:</p>
                <pre><code class="language-c">int sum = 42 + 8;</code></pre>
                <p>The lexer produces tokens:</p>
                <table class="table table-bordered table-striped">
                    <tr>
                        <th>Lexeme</th>
                        <th>Token Name</th>
                        <th>Attribute</th>
                    </tr>
                    <tr>
                        <td>int</td>
                        <td>KEYWORD</td>
                        <td>int</td>
                    </tr>
                    <tr>
                        <td>sum</td>
                        <td>IDENTIFIER</td>
                        <td>Pointer to symbol table</td>
                    </tr>
                    <tr>
                        <td>=</td>
                        <td>OPERATOR</td>
                        <td>=</td>
                    </tr>
                    <tr>
                        <td>42</td>
                        <td>LITERAL</td>
                        <td>Integer</td>
                    </tr>
                    <tr>
                        <td>+</td>
                        <td>OPERATOR</td>
                        <td>+</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>LITERAL</td>
                        <td>Integer</td>
                    </tr>
                    <tr>
                        <td>;</td>
                        <td>SPECIAL_SYMBOL</td>
                        <td>Statement Terminator</td>
                    </tr>
                </table>

                <h4>Challenges in Tokenization</h4>
                <ul>
                    <li><strong>Lookahead Issues</strong>: Differentiating between <code>=</code> and <code>==</code> requires a two-character lookahead.</li>
                    <li><strong>Whitespace Handling</strong>: Spaces are ignored except inside string literals.</li>
                    <li><strong>Handling Escape Sequences</strong>: Recognizing <code>\n</code>, <code>\t</code> in strings.</li>
                    <li><strong>Multi-line Tokens</strong>: Managing multi-line comments and string literals.</li>
                </ul>

            </article>
            <article>
                <h4>Regular Expressions & Finite Automata</h4>
                <p>Lexical analyzers use Regular Expressions (RE) and Finite Automata (FA) to define and recognize token patterns in programming languages. These mathematical models efficiently describe how characters are grouped into meaningful tokens, ensuring correct identification of keywords, literals, identifiers, and operators.</p>

                <h4>Regular Expressions (RE)</h4>
                <p>A Regular Expression (RE) is a sequence of characters that defines a search pattern. It is a formal way to describe patterns in strings and is widely used in lexical analysis.</p>

                <h4>Components of Regular Expressions</h4>
                <ul>
                    <li><strong>Alphabet (Σ)</strong>: A finite set of symbols. For example, in programming, Σ can be the ASCII character set.</li>
                    <li><strong>Concatenation</strong>: Joining two patterns sequentially. Example: <code>ab</code> matches "ab".</li>
                    <li><strong>Union (Alternation)</strong>: Represents multiple choices. Example: <code>a|b</code> matches "a" or "b".</li>
                    <li><strong>Kleene Star (*)</strong>: Represents zero or more occurrences. Example: <code>a*</code> matches "", "a", "aa", "aaa", etc.</li>
                    <li><strong>Plus Operator (+)</strong>: One or more occurrences. Example: <code>a+</code> matches "a", "aa", "aaa", etc., but not "".</li>
                    <li><strong>Optional Operator (?)</strong>: Zero or one occurrence. Example: <code>a?</code> matches "" or "a".</li>
                    <li><strong>Character Classes</strong>: Defines a set of characters. Example: <code>[0-9]</code> matches any digit.</li>
                </ul>

                <h4>Example Regular Expressions</h4>
                <table class="table table-bordered table-striped">
                    <tr>
                        <th>Pattern</th>
                        <th>Description</th>
                        <th>Example Matches</th>
                    </tr>
                    <tr>
                        <td><code>[a-zA-Z_][a-zA-Z0-9_]*</code></td>
                        <td>Valid Identifiers</td>
                        <td>var_name, _temp, count1</td>
                    </tr>
                    <tr>
                        <td><code>[0-9]+</code></td>
                        <td>Integer Numbers</td>
                        <td>42, 123, 9999</td>
                    </tr>
                    <tr>
                        <td><code>[0-9]*\.[0-9]+</code></td>
                        <td>Floating-point Numbers</td>
                        <td>3.14, 0.5, .75</td>
                    </tr>
                    <tr>
                        <td><code>"([^"\\]|\\.)*"</code></td>
                        <td>String Literals</td>
                        <td>"hello", "C Compiler"</td>
                    </tr>
                    <tr>
                        <td><code>(if|while|for|return)</code></td>
                        <td>Keywords</td>
                        <td>if, while, return</td>
                    </tr>
                    <tr>
                        <td><code>\+\+|\-|\*|\/</code></td>
                        <td>Operators</td>
                        <td>++, -, *, /</td>
                    </tr>
                </table>

                <h4>Finite Automata (FA)</h4>
                <p>Finite Automata are abstract machines used to implement lexical analyzers that recognize patterns defined by regular expressions. There are two main types:</p>

                <h4>Deterministic Finite Automata (DFA)</h4>
                <p>A Deterministic Finite Automaton (DFA) is a state machine where each state has exactly one transition per input symbol.</p>
                <ul>
                    <li><strong>Advantages</strong>: Fast execution, as there is no need for backtracking.</li>
                    <li><strong>Disadvantages</strong>: Requires more memory due to larger state tables.</li>
                    <li><strong>Example DFA for Identifiers:</strong></li>
                </ul>

                <pre><code class="language-mermaid d-none">
stateDiagram
    [*] --> S0
    S0 --> S1 : [a-zA-Z_]
    S1 --> S1 : [a-zA-Z0-9_]
    S1 --> [*] : Accept (Identifier)
</code></pre>
                <div class="text-center">
                    <img class="img-fluid dynamicimg imgblacktowhite" loading="lazy" src="https://mermaid.ink/img/pako:eNplUMFugzAM_ZXIp20C5IQSIIdJlXrpYSduLdUUQQqRCkFpkNYi_n0pdJq0-eSn9_xsvwkqUysQcHXSqZ2WjZVd2RNfx7cTCcN3UuCKC1whJYIcZXjfhofP05Oi_ygM8z_sw0-QbVWpwZGXfa16p89a2VcIoFO2k7r2d0yPmRJcqzpVgvDtRTetK6HsZy-UozPFra9AODuqAKwZmxbEWV6uHo1D_fvFj2SQPYgJvkBwGtE8iRNMKY8R400ANxCUYURpsmF5ypFhlmVzAHdjvAFGKaMpzxhHREZjzha7w0I-d6paO2M_1hSXMOdvduBhog?type=png" />
                </div>

                <p>Explanation:</p>
                <ul>
                    <li>S0: Start state.</li>
                    <li>S1: Moves to an identifier state if it starts with a letter or underscore.</li>
                    <li>S1 (looping): Accepts alphanumeric characters and underscores.</li>
                </ul>

                <h4>Nondeterministic Finite Automata (NFA)</h4>
                <p>A Nondeterministic Finite Automaton (NFA) allows multiple transitions for the same input, meaning multiple states can be active at once.</p>
                <ul>
                    <li><strong>Advantages</strong>: More compact representation.</li>
                    <li><strong>Disadvantages</strong>: Requires backtracking or additional computation.</li>
                    <li><strong>Example NFA for Identifiers:</strong></li>
                </ul>

                <pre><code class="language-mermaid d-none">
graph TD
    A[Start] -->|a-z, A-Z, _| B
    B -->|a-z, A-Z, 0-9, _| B
    B -->|ε| Accept
</code></pre>
                <div class="text-center">
                    <img class="img-fluid dynamicimg imgblacktowhite" loading="lazy" src="https://mermaid.ink/img/pako:eNplkEFugzAQRa9izdpGYxMMeFGJKNuu0lVKVVngAFLAyDVSE-BYvUbPVJdUXbSzmpn_9Gf0Z6hsbUBB4_TYkqdDOZBQxfPRa-dfCGMPi2Y3Sgp2ouR1Ifs7sP-jIMv_y58fCymqyoweKPTG9bqrw6n5mynBt6Y3JajQXrqm9SWUwxpAPXl7vA4VKO8mQ8HZqWlBnfXlLUzTWGtvDp0OD_e_21EPoGZ4ByV5xPMkTjDlMkaMdxSuoLjAiPNkJ_JUosAsy1YKN2uDA0ap4KnMhEREwWMpNrvTJv7Ym7rz1j3ek9oCW78ARIZbiQ?type=png" />
                </div>

                <h4>Converting NFA to DFA</h4>
                <p>An NFA can be converted into an equivalent DFA using the Subset Construction Algorithm. This process removes ambiguity by constructing deterministic states from multiple NFA states.</p>
                <ul>
                    <li><strong>Step 1</strong>: Identify ε-closures (states reachable without consuming input).</li>
                    <li><strong>Step 2</strong>: Construct DFA states from NFA states.</li>
                    <li><strong>Step 3</strong>: Define transitions uniquely for each input.</li>
                </ul>

                <h4>NFA vs. DFA: Comparison</h4>
                <table class="table table-bordered table-striped">
                    <tr>
                        <th>Feature</th>
                        <th>NFA</th>
                        <th>DFA</th>
                    </tr>
                    <tr>
                        <td>Number of transitions per state</td>
                        <td>Multiple allowed</td>
                        <td>One per input</td>
                    </tr>
                    <tr>
                        <td>Backtracking</td>
                        <td>Possible</td>
                        <td>Not required</td>
                    </tr>
                    <tr>
                        <td>Speed</td>
                        <td>Slower</td>
                        <td>Faster</td>
                    </tr>
                    <tr>
                        <td>Memory Usage</td>
                        <td>Lower</td>
                        <td>Higher</td>
                    </tr>
                    <tr>
                        <td>Conversion Complexity</td>
                        <td>Simple to construct</td>
                        <td>Requires subset construction</td>
                    </tr>
                </table>

                <h4>Using FA in Lexical Analysis</h4>
                <ul>
                    <li>Each token type (keywords, identifiers, literals) is defined using a regular expression.</li>
                    <li>The RE is converted into an NFA, which allows multiple paths.</li>
                    <li>The NFA is then converted into a DFA to ensure efficient, backtrack-free scanning.</li>
                    <li>The DFA is implemented in a scanner generator (e.g., Lex, Flex) or manually coded in the compiler.</li>
                </ul>

                <h4>Industry Applications</h4>
                <ul>
                    <li><strong>Compilers</strong>: Tokenization in GCC, Clang, Java Compiler (Javac).</li>
                    <li><strong>Text Editors</strong>: Syntax highlighting in VS Code, Sublime Text.</li>
                    <li><strong>Search Engines</strong>: Pattern matching in Google Search.</li>
                    <li><strong>Network Security</strong>: Intrusion detection systems using regex filtering.</li>
                </ul>

            </article>

            <article>
                <h3>Lexical Errors</h3>
                <p>Lexical errors occur when an input sequence of characters does not match any valid token pattern defined by the programming language's lexical rules. These errors are detected at the earliest stage of compilation (lexical analysis) and can prevent further processing.</p>

                <h4>Importance of Detecting Lexical Errors</h4>
                <ul>
                    <li>Ensures syntax correctness before parsing begins.</li>
                    <li>Prevents ambiguities in token recognition.</li>
                    <li>Helps in debugging and error reporting.</li>
                    <li>Essential for code editors and IDEs to provide real-time feedback.</li>
                </ul>

                <h4>Causes of Lexical Errors</h4>
                <p>Lexical errors arise due to:</p>
                <ul>
                    <li><strong>Use of illegal characters</strong> that do not belong to the language's character set.</li>
                    <li><strong>Malformed identifiers</strong> that violate naming conventions.</li>
                    <li><strong>Incorrect literals</strong> such as unclosed strings or malformed numbers.</li>
                    <li><strong>Unexpected tokens</strong> that break tokenization rules.</li>
                </ul>

                <h4>Common Types of Lexical Errors</h4>

                <h5>Illegal Characters</h5>
                <p>Characters that are not part of the programming language's character set result in lexical errors.</p>

                <h6>Examples</h6>
                <ul>
                    <li><strong>Example in C:</strong> The <code>#</code> symbol is reserved for preprocessor directives but cannot appear in regular expressions.</li>
                    <pre><code class="language-c">
int a = 10 # 5;  // Error: Unexpected #
</code></pre>

                    <li><strong>Example in Java:</strong> The <code>@</code> symbol is only allowed for annotations, not as an identifier character.</li>
                    <pre><code class="language-java">
int @var = 5;  // Error: Illegal character '@'
</code></pre>

                    <li><strong>Example in Python:</strong> The <code>$</code> symbol is not valid in variable names.</li>
                    <pre><code class="language-python">
$amount = 100  # Error: Unexpected '$' in identifier
</code></pre>
                </ul>

                <h5>Invalid Identifiers</h5>
                <p>Identifiers must follow specific naming conventions:</p>
                <ul>
                    <li>Cannot start with a digit (e.g., <code>1var</code> is invalid in most languages).</li>
                    <li>Must not contain special characters (e.g., <code>my-variable</code> is invalid in C but valid in JavaScript).</li>
                    <li>Cannot use reserved keywords as variable names.</li>
                </ul>

                <h6>Examples</h6>
                <pre><code class="language-cpp">
// Incorrect
int 1value = 10;  // Error: Identifier cannot start with a digit

// Incorrect
int if = 5;  // Error: 'if' is a reserved keyword
</code></pre>

                <h5>Unclosed String Literals</h5>
                <p>String literals must start and end with the correct delimiter, such as <code>"</code> or <code>'</code>. If the closing delimiter is missing, a lexical error occurs.</p>

                <h6>Examples</h6>
                <pre><code class="language-c">
// Incorrect: Missing closing quote
printf("Hello world);  // Error: Unterminated string literal
</code></pre>

                <pre><code class="language-python">
// Incorrect: Unmatched single quote
print('Welcome)  // Error: Unclosed string literal
</code></pre>

                <h5>Invalid Numeric Literals</h5>
                <p>Numbers must follow proper formatting rules.</p>

                <h6>Examples</h6>
                <ul>
                    <li><strong>Example (Incorrect Integer):</strong> Leading zeros can be interpreted as octal in some languages.</li>
                    <pre><code class="language-c">
int num = 05;  // Warning: Leading zero indicates an octal number
</code></pre>

                    <li><strong>Example (Incorrect Floating-Point Number):</strong> Double decimal points are not allowed.</li>
                    <pre><code class="language-c">
float x = 3..14;  // Error: Unexpected '.'
</code></pre>
                </ul>

                <h5>Invalid Escape Sequences</h5>
                <p>Escape sequences are used inside string literals to represent special characters, but an incorrect escape sequence can cause a lexical error.</p>

                <h6>Examples</h6>
                <pre><code class="language-java">
// Incorrect: \q is not a valid escape sequence
System.out.println("Hello \q World");  // Error
</code></pre>

                <pre><code class="language-python">
// Incorrect: \U starts a Unicode sequence but is incomplete
print("C:\Users\John")  // Error: Incomplete Unicode escape sequence
</code></pre>

                <h5>Unterminated Comments</h5>
                <p>Multi-line comments must be properly closed; otherwise, the lexer will reach the end of the file without finding the closure.</p>

                <h6>Examples</h6>
                <pre><code class="language-c">
/* This is a comment 
printf("Hello, World!");  // Error: Missing '*/'
</code></pre>

                <h4>Detecting and Handling Lexical Errors</h4>
                <p>The lexical analyzer (lexer) detects errors using Finite Automata and follows these strategies:</p>

                <ul>
                    <li><strong>Error Reporting:</strong> The compiler highlights the position of the error.</li>
                    <li><strong>Error Recovery:</strong> The lexer may attempt to correct minor mistakes (e.g., replacing invalid characters).</li>
                    <li><strong>Skipping Unrecognized Tokens:</strong> The lexer may ignore invalid input and continue processing.</li>
                </ul>

                <h6>Example: Lexical Error Messages</h6>
                <pre><code class="language-java">
int 1number = 10;
System.out.println("Hello World);
</code></pre>

                <p>The compiler output may be:</p>
                <pre><code>
Error: Invalid identifier '1number' (line 1, column 5)
Error: Unterminated string literal (line 2, column 27)
</code></pre>

                <h4>Real-World Applications</h4>
                <ul>
                    <li><strong>Compilers (GCC, Clang, Javac):</strong> Detect lexical errors during preprocessing.</li>
                    <li><strong>IDEs (VS Code, IntelliJ):</strong> Provide real-time lexical error highlighting.</li>
                    <li><strong>Syntax Highlighters:</strong> Detect unclosed strings and invalid tokens.</li>
                    <li><strong>Static Code Analysis Tools (ESLint, PyLint):</strong> Identify invalid identifiers and symbols.</li>
                </ul>

                <h4>Summary of Lexical Errors</h4>
                <table class="table table-bordered table-striped">
                    <tr>
                        <th>Lexical Error Type</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td>Illegal Characters</td>
                        <td>Characters not in the language.</td>
                        <td><code>int x = 10 @ 5;</code> (Error: '@')</td>
                    </tr>
                    <tr>
                        <td>Invalid Identifiers</td>
                        <td>Incorrect variable names.</td>
                        <td><code>int 1var = 5;</code> (Error: Cannot start with a digit)</td>
                    </tr>
                    <tr>
                        <td>Unclosed Strings</td>
                        <td>Missing string delimiters.</td>
                        <td><code>print("Hello;</code> (Error: Unterminated string)</td>
                    </tr>
                    <tr>
                        <td>Invalid Numbers</td>
                        <td>Incorrect numeric formats.</td>
                        <td><code>float x = 3..14;</code> (Error: Double dots)</td>
                    </tr>
                    <tr>
                        <td>Unterminated Comments</td>
                        <td>Comment block not closed.</td>
                        <td><code>/* This is a comment</code> (Error: Missing '*/')</td>
                    </tr>
                </table>

            </article>


            <article>
                <h3>Lexical Error Recovery Techniques</h3>
                <p>Lexical error recovery ensures that minor errors in the source code do not halt the compilation process completely. Instead of stopping execution on encountering an invalid token, modern compilers attempt to recover and continue processing the remaining code. This improves robustness and provides useful feedback to the programmer.</p>

                <h4>Types of Lexical Errors</h4>
                <p>Lexical errors occur when a sequence of characters does not match any valid token pattern defined by the language. Common lexical errors include:</p>
                <ul>
                    <li>Illegal Characters: Characters not allowed in the language (e.g., `@` in C).</li>
                    <li>Invalid Identifiers: Variable names starting with a digit (e.g., `1var`).</li>
                    <li>Unclosed String Literals: Missing closing quotation marks (`"Hello`).</li>
                    <li>Invalid Numeric Formats: Floating-point errors like `5..5`.</li>
                    <li>Unknown Symbols: Special characters used incorrectly (`$value` in Java).</li>
                </ul>

                <h4>Panic Mode Recovery</h4>
                <p>The simplest and most commonly used method where the lexer discards characters until a valid token is found.</p>

                <h5>Example</h5>
                <pre><code class="language-c">
int x = 5 @ 10;
</code></pre>
                <p>In this case, `@` is an illegal character in C.</p>
                <ul>
                    <li>Lexer Action: Skip `@` and continue scanning.</li>
                    <li>Recovered Output: The lexer recognizes `&lt;int>
                        &lt;x> = &lt;5>
                        &lt;10>;` but reports an error at `@`.</li>
                </ul>

                <h5>Use Case</h5>
                <ul>
                    <li>Ideal for unrecognized symbols in the middle of a statement.</li>
                    <li>Prevents the compiler from halting on a minor error.</li>
                </ul>

                <h4>Error Token Insertion</h4>
                <p>The lexer replaces an invalid token with a placeholder (e.g., `&lt;TOKEN_ERROR>`) and continues scanning.</p>

                <h5>Example</h5>
                <pre><code class="language-python">1var = 10</code></pre>
                <p>In Python, identifiers cannot start with a digit.</p>
                <ul>
                    <li>Lexer Action: Replace `1var` with `&lt;TOKEN_ERROR>` but allow further parsing.</li>
                    <li>Recovered Output: `&lt;TOKEN_ERROR> = 10`</li>
                </ul>

                <h5>Use Case</h5>
                <ul>
                    <li>Useful when handling minor lexing mistakes.</li>
                    <li>Ensures that parsing can continue while marking the erroneous token.</li>
                </ul>

                <h4>Symbol Table Correction</h4>
                <p>For minor lexical mistakes, the lexer may suggest corrections based on the symbol table.</p>

                <h5>Example</h5>
                <pre><code class="language-java">
whil (x < 10) {
    System.out.println(x);
}
</code></pre>
                <p>The keyword `whil` is misspelled, but the lexer detects it as a probable `while`.</p>
                <ul>
                    <li>Lexer Action: Suggest `&lt;while>` instead of `whil`.</li>
                    <li>Recovered Output: `while (x < 10) { System.out.println(x); }`</li>
                </ul>

                <h5>Use Case</h5>
                <ul>
                    <li>Helps in typo detection for keywords and identifiers.</li>
                    <li>Common in IDEs and modern compilers with autocorrect features.</li>
                </ul>

                <h4>Lookahead Correction</h4>
                <p>The lexer looks ahead to check if an error can be automatically corrected.</p>

                <h5>Example</h5>
                <pre><code class="language-c">
int x = 5.5.3;
</code></pre>
                <p>This is invalid because `5.5.3` is not a valid floating-point number.</p>
                <ul>
                    <li>Lexer Action: Identify `5.5` as a valid floating-point number and `.3` as a separate token.</li>
                    <li>Recovered Output: `int x = 5.5; .3;` (which may cause further errors in parsing but avoids lexical failure).</li>
                </ul>

                <h5>Use Case</h5>
                <ul>
                    <li>Effective for handling token boundary confusion.</li>
                    <li>Ensures tokens are correctly split instead of marking the whole sequence invalid.</li>
                </ul>

                <h4>Logging & Reporting</h4>
                <p>Modern compilers maintain error logs instead of immediately stopping execution on lexical errors.</p>

                <h5>Example</h5>
                <p>In VS Code or IntelliJ, an incomplete statement like:</p>
                <pre><code class="language-javascript">
console.log("Hello;
</code></pre>
                <p>Shows a warning: `Unclosed string literal` but allows further typing.</p>

                <h5>Use Case</h5>
                <ul>
                    <li>Helps developers debug efficiently.</li>
                    <li>Widely used in modern IDEs and error-tracking systems.</li>
                </ul>

                <h4>Real-World Applications</h4>
                <ul>
                    <li><strong>Compilers (GCC, Clang):</strong> Implement panic mode recovery to skip unknown symbols.</li>
                    <li><strong>IDEs (VS Code, IntelliJ):</strong> Use symbol table correction and logging to highlight errors.</li>
                    <li><strong>Interpreters (Python, JavaScript Engines):</strong> Apply lookahead correction to split ambiguous tokens.</li>
                    <li><strong>Static Analysis Tools (ESLint, PyLint):</strong> Use error token insertion to continue checking code.</li>
                </ul>

                <h4>Summary</h4>
                <table class="table table-bordered table-striped">
                    <tr>
                        <th>Recovery Technique</th>
                        <th>Action Taken</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td>Panic Mode Recovery</td>
                        <td>Skip invalid tokens until a valid token is found.</td>
                        <td>`int x = 5 @ 10;` → Skips `@`.</td>
                    </tr>
                    <tr>
                        <td>Error Token Insertion</td>
                        <td>Replace invalid tokens with `&lt;TOKEN_ERROR>`.</td>
                        <td>`1var = 10` → `&lt;TOKEN_ERROR> = 10`.</td>
                    </tr>
                    <tr>
                        <td>Symbol Table Correction</td>
                        <td>Suggests possible corrections for mistyped keywords.</td>
                        <td>`whil (x < 10)` → Suggests `while`.</td>
                    </tr>
                    <tr>
                        <td>Lookahead Correction</td>
                        <td>Splits ambiguous tokens correctly.</td>
                        <td>`int x = 5.5.3;` → Splits into `5.5` and `.3`.</td>
                    </tr>
                    <tr>
                        <td>Logging & Reporting</td>
                        <td>Logs lexical errors without stopping execution.</td>
                        <td>VS Code highlights `console.log("Hello;` as an error.</td>
                    </tr>
                </table>

            </article>


            </article>


            <article>
                <h3>Real-World Applications</h3>
                <p>Lexical Analysis is widely used beyond compiler design. Many real-world systems and software applications depend on efficient tokenization, pattern recognition, and lexical scanning to process and understand textual data. Below are some key areas where lexical analysis plays a crucial role.</p>

                <h4>Code Syntax Highlighting</h4>
                <p>Lexical analysis is used in Integrated Development Environments (IDEs) and text editors to recognize different programming constructs and apply appropriate syntax highlighting.</p>

                <h5>How it works:</h5>
                <ul>
                    <li>The lexer scans the source code and classifies tokens into categories such as keywords, identifiers, literals, and operators.</li>
                    <li>Each token is assigned a color or style based on predefined rules.</li>
                    <li>The syntax highlighter updates dynamically as the user types.</li>
                </ul>

                <h5>Examples:</h5>
                <ul>
                    <li><strong>Visual Studio Code</strong>: Uses TextMate grammars to recognize token patterns.</li>
                    <li><strong>PyCharm</strong>: Implements lexical analysis for real-time syntax validation.</li>
                    <li><strong>Sublime Text</strong>: Uses regex-based lexers to highlight syntax.</li>
                </ul>

                <h5>Example:</h5>
                <p>Consider this Python snippet in an IDE:</p>
                <pre><code class="language-python">
def greet(name):
    print("Hello, " + name + "!")
</code></pre>
                <p>The lexer categorizes and highlights:</p>
                <ul>
                    <li><code>def</code> (Keyword - Blue)</li>
                    <li><code>greet</code> (Function Name - White)</li>
                    <li><code>name</code> (Parameter - White)</li>
                    <li><code>print</code> (Built-in Function - Green)</li>
                    <li><code>"Hello, "</code> (String Literal - Yellow)</li>
                </ul>

                <h4>Keyword Extraction in Search Engines</h4>
                <p>Search engines such as Google and Bing use lexical analysis to extract meaningful words from user queries and match them against indexed web pages.</p>

                <h5>How it works:</h5>
                <ul>
                    <li>Lexical analysis splits the user’s query into tokens (words, numbers, special characters).</li>
                    <li>Stop words (e.g., "the", "a", "of") are removed to focus on key terms.</li>
                    <li>Lexical scanning applies stemming and lemmatization (e.g., "running" → "run").</li>
                    <li>The parsed tokens are compared with indexed content for search ranking.</li>
                </ul>

                <h5>Examples:</h5>
                <ul>
                    <li><strong>Google Search</strong>: Uses tokenization and stemming to optimize search results.</li>
                    <li><strong>Bing Search</strong>: Applies lexical analysis to analyze user intent.</li>
                    <li><strong>DuckDuckGo</strong>: Uses lexical parsing to avoid redundant processing.</li>
                </ul>

                <h5>Example:</h5>
                <p>Consider a user searching for:</p>
                <pre><code>
"Best places to visit in Paris during winter"
</code></pre>
                <p>The lexer processes it as:</p>
                <ul>
                    <li><strong>Keywords:</strong> "best", "places", "visit", "Paris", "winter"</li>
                    <li><strong>Stop Words Removed:</strong> "to", "in", "during"</li>
                    <li><strong>Indexed Matching:</strong> Results are ranked based on keyword relevance.</li>
                </ul>

                <h4>Command-Line Interpreters</h4>
                <p>Lexical analyzers are essential in command-line interpreters like Bash, PowerShell, and Zsh to process user commands efficiently.</p>

                <h5>How it works:</h5>
                <ul>
                    <li>The lexer reads the user input character by character.</li>
                    <li>The command is broken into tokens (command, options, arguments).</li>
                    <li>The parsed tokens are sent to the shell for execution.</li>
                </ul>

                <h5>Examples:</h5>
                <ul>
                    <li><strong>Bash Shell</strong>: Tokenizes input like <code>ls -l /home</code>.</li>
                    <li><strong>PowerShell</strong>: Recognizes commands like <code>Get-Process | Sort-Object CPU</code>.</li>
                    <li><strong>Zsh</strong>: Uses lexical analysis for auto-completion and syntax validation.</li>
                </ul>

                <h5>Example:</h5>
                <p>Consider this Linux command:</p>
                <pre><code class="language-bash">
grep "error" logfile.txt | sort | uniq -c
</code></pre>
                <p>The lexer tokenizes the input as:</p>
                <ul>
                    <li><code>grep</code> (Command - searches for a pattern in files)</li>
                    <li><code>"error"</code> (String Literal - search term)</li>
                    <li><code>logfile.txt</code> (Filename - argument)</li>
                    <li><code>|</code> (Pipe Operator - passes output to next command)</li>
                    <li><code>sort</code> (Command - sorts lines)</li>
                    <li><code>uniq -c</code> (Command - removes duplicates and counts occurrences)</li>
                </ul>

                <h4>Additional Real-World Applications</h4>
                <ul>
                    <li><strong>Text Editors (MS Word, Google Docs):</strong> Use lexical analysis for spell checking and grammar correction.</li>
                    <li><strong>Natural Language Processing (NLP):</strong> Chatbots and AI assistants like Siri, Alexa, and Google Assistant tokenize spoken words for interpretation.</li>
                    <li><strong>Data Processing Tools:</strong> Big Data frameworks like Apache Spark and Hadoop tokenize unstructured data before analysis.</li>
                    <li><strong>Log Analysis Tools:</strong> Security software like Splunk parses logs using lexical scanners to detect anomalies.</li>
                </ul>
            </article>


            <article>
                <h3>Common Pitfalls & Debugging</h3>
                <p>Lexical analysis, while crucial for efficient compilation, has several challenges that can cause incorrect tokenization, performance issues, or incompatibility with modern programming needs. Below are common pitfalls in lexical analysis and their debugging strategies.</p>

                <h4>Handling Whitespace & Comments Efficiently</h4>
                <p>Whitespace and comments do not contribute to program execution but must be handled carefully during lexical analysis to avoid inefficiencies.</p>

                <h5>Problems Caused by Whitespace & Comments</h5>
                <ul>
                    <li>Unnecessary processing increases compilation time.</li>
                    <li>Multi-line comments may cause unintentional nesting errors.</li>
                    <li>Inconsistent tab and space handling can alter code behavior in whitespace-sensitive languages (e.g., Python).</li>
                </ul>

                <h5>Strategies for Efficient Handling</h5>
                <ul>
                    <li><strong>Ignore Redundant Whitespace:</strong> The lexer should discard unnecessary spaces except inside string literals.</li>
                    <li><strong>Efficient Comment Removal:</strong> Single-line (<code>//</code>) and multi-line (<code>/*...*/</code>) comments should be skipped early.</li>
                    <li><strong>Lookahead for Block Comments:</strong> Ensure multi-line comments properly close before proceeding.</li>
                    <li><strong>Optimize Buffering:</strong> Use buffered reading to handle large files efficiently.</li>
                </ul>

                <h5>Example Issue & Debugging</h5>
                <p>Consider this JavaScript code:</p>
                <pre><code class="language-javascript">
let x =  10;  // This is a comment
console.log("Value: " + x);
</code></pre>
                <p>If the lexer fails to remove comments properly, the comment may interfere with parsing. A debugging strategy involves:</p>
                <ul>
                    <li>Checking if the comment tokenization stops at `\n` for single-line comments.</li>
                    <li>Ensuring `/*` is closed by `*/` in multi-line comments.</li>
                </ul>

                <h4>Resolving Ambiguous Tokens</h4>
                <p>Lexers must correctly distinguish between tokens that share prefixes (e.g., <code>&lt;=</code> vs. <code>&lt; =</code>).</p>

                <h5>1. Why Ambiguity Occurs</h5>
                <ul>
                    <li>Some operators overlap (e.g., <code>=</code> vs. <code>==</code>).</li>
                    <li>Spaces may or may not separate tokens.</li>
                    <li>Lexical lookahead is required to decide token boundaries.</li>
                </ul>

                <h5>Example of Token Ambiguity</h5>
                <p>Consider this C++ expression:</p>
                <pre><code class="language-cpp">
if (x <= y)
if (x < = y)
</code></pre>
                <p>The first line correctly recognizes <code>&lt;=</code> as a single token, but the second line has ambiguity:</p>
                <ul>
                    <li>Should it be interpreted as `<` and `=` separately?</li>
                    <li>Or should the lexer expect a space-separated `=`?</li>
                </ul>

                <h5>Debugging Strategy</h5>
                <ul>
                    <li>Use greedy matching (prefer longest token first).</li>
                    <li>Implement multi-character lookahead.</li>
                    <li>Maintain a token buffer to store partially recognized sequences.</li>
                </ul>

                <h5>Solution in a DFA</h5>
                <p>To resolve `<=` vs. `<=`, the DFA transitions should be structured as:</p>
                        <pre><code class="language-mermaid d-none">
stateDiagram
    [*] --> LESS : '<'
    LESS --> LESSEQUAL : '='
    LESS --> [*] : Accept '<'
    LESSEQUAL --> [*] : Accept '<='
</code></pre>
                        <div class="text-center">
                            <img class="img-fluid dynamicimg imgblacktowhite" loading="lazy" src="https://mermaid.ink/img/pako:eNptUEtrg0AQ_ivLXIRgZHaNq1mSQqC5pYcivbTbw6JbFaIrZoWm4n_vqn2Q0jnNN9-DmRkgM7kGARerrL6vVNGpWjbE1cvqlazXd-R0TFMiiLfzlvmMv4nj49PhNLH7v-xkF-SQZbq1t-bF849m74EPte5qVeVupWGySLClrrUE4dpzVZRWgmxGJ1S9Nem1yUDYrtc-dKYvShBv6nxxqG_z34N-pq1qQAzwDoLTgG6jMMKY8hAx3PhwBUEZBpRGG7aNOTJMkmT04cMYl4BBzGjME8YRkdGQsznueSa_4nVeWdM9LB-dHzt-AoEyY1Q?type=png" />
                        </div>

                        <h4>Dealing with Unicode & Multi-Language Lexing</h4>
                        <p>Modern compilers need to support Unicode characters, which introduces complexity in tokenization.</p>

                        <h5>Challenges in Unicode Handling</h5>
                        <ul>
                            <li><strong>Identifiers in non-English languages:</strong> Programming languages like Python allow Unicode variable names.</li>
                            <li><strong>Byte Order Mark (BOM) Issues:</strong> Some UTF-encoded files may start with a BOM, which needs to be ignored.</li>
                            <li><strong>Multi-byte Characters:</strong> UTF-8 characters can span multiple bytes, requiring careful handling.</li>
                        </ul>

                        <h5>Examples of Unicode in Identifiers</h5>
                        <p>Valid variable names in Python:</p>
                        <pre><code class="language-python">
变量 = 100  # Chinese variable name
π = 3.14    # Greek letter as identifier
</code></pre>

                        <h5>Debugging Strategies</h5>
                        <ul>
                            <li><strong>Use Unicode-aware tokenization:</strong> Extend identifier regex to include Unicode ranges.</li>
                            <li><strong>Normalize Unicode input:</strong> Convert all input to a standard form (e.g., NFC normalization).</li>
                            <li><strong>Handle UTF-8 correctly:</strong> Ensure lexers read multi-byte sequences properly.</li>
                        </ul>

                        <h5>Example Regex for Unicode Identifiers</h5>
                        <pre><code class="language-regex">
[\p{L}_][\p{L}\p{N}_]*
</code></pre>
                        <p>This allows letters from any language in identifiers.</p>

                        <h5>Real-World Applications</h5>
                        <ul>
                            <li><strong>Python:</strong> Supports Unicode variable names.</li>
                            <li><strong>JavaScript:</strong> Uses UTF-16 internally.</li>
                            <li><strong>Modern Editors (VS Code):</strong> Syntax highlighters recognize Unicode tokens.</li>
                        </ul>
            </article>

            <article>
                <h3>Advanced Optimization for Industry Use</h3>
                <p>Optimizing lexical analysis is essential in industry applications where performance, memory efficiency, and speed are critical. Techniques such as DFA minimization and Just-In-Time (JIT) lexing help reduce overhead and improve real-time execution in modern compilers and interpreters.</p>

                <h4>DFA Minimization for Fast Scanning</h4>
                <p>Deterministic Finite Automaton (DFA) minimization is an optimization technique that reduces the number of states in a lexical analyzer, making token recognition faster and more efficient.</p>

                <h5>Why DFA Minimization Matters</h5>
                <ul>
                    <li>Large DFAs can cause high memory usage and slow execution.</li>
                    <li>Minimization reduces redundant states, making scanning faster.</li>
                    <li>Fewer states mean fewer computations per character.</li>
                </ul>

                <h5>Steps in DFA Minimization</h5>
                <p>The Hopcroft's Algorithm is widely used for minimizing DFAs:</p>
                <ul>
                    <li><strong>Step 1: Partitioning States</strong>
                        <ul>
                            <li>Group states into accepting and non-accepting partitions.</li>
                        </ul>
                    </li>
                    <li><strong>Step 2: Splitting Equivalent States</strong>
                        <ul>
                            <li>States that behave identically (same transitions) are merged.</li>
                        </ul>
                    </li>
                    <li><strong>Step 3: Constructing the Minimal DFA</strong>
                        <ul>
                            <li>Eliminate unnecessary transitions.</li>
                            <li>Ensure each state is reachable.</li>
                        </ul>
                    </li>
                </ul>

                <h5>Example: Minimizing a DFA</h5>
                <p>Consider a DFA that recognizes the keywords "if" and "int".</p>

                <h5>Before Minimization:</h5>
                <pre><code class="language-mermaid d-none">
stateDiagram
    [*] --> S0
    S0 --> S1 : 'i'
    S1 --> S2 : 'f'
    S1 --> S3 : 'n'
    S3 --> S4 : 't'
    S2 --> [*] : Accept 'if'
    S4 --> [*] : Accept 'int'
</code></pre>
                <div class="text-center">
                    <img class="img-fluid dynamicimg imgblacktowhite" loading="lazy" src="https://mermaid.ink/img/pako:eNptkMFqhDAQhl8lzEUorkwSjW4OhUKvPXlr7SFoVgNrIm6EbsV3b9TdLZTmlP-b4ZtkZqhdo0HCxSuvX41qR9VXloTz8fRJDodnUuKeS9wjJZJEJrpBukO2wtMfyFdo75DvMF2hv0O2wXWUJC91rQcf3A9P-l_V-ghi6PXYK9OEl89rcwW-072uQIbr2bSdr6CyS2hUk3fl1dYg_TjpGEY3tR3IkzpfQpqG5vffDzooC3KGL5CCJvSY8QxzKjgiT2O4gqQME0qzlB1zgQyLolhi-HYuGDDJGc1FwQQiMsoF23TvW_Gm143xbnzbF7_tf_kBxhxqmg?type=png" />
                </div>


                <h5>After Minimization:</h5>
                <pre><code class="language-mermaid d-none">
stateDiagram
    [*] --> S0
    S0 --> S1 : 'i'
    S1 --> S2 : 'f' (if) / 'n' (int)
    S2 --> [*] : Accept (if/int)
</code></pre>
                <div class="text-center">
                    <img class="img-fluid dynamicimg imgblacktowhite" loading="lazy" src="https://mermaid.ink/img/pako:eNpFkEFvgzAMhf9K5AvrRKkTSqA5TJq0607cNnaIIECkkiAapHWI_74Eqs0nv-dPT7YXqG2jQMDNSafetOwmOVSG-Pp8_iLh3wspcdcl7pISQSIdPUy6myyYbUSedHsgJxKZ0Bp3eFBso0KiIK91rUYXyFMgIIZBTYPUjd9iCXwFrleDqkD49qq73lVQmdWDcna2vJsahJtmFcNk564H0crrzat5bP5v-HNHaUAs8A2C04ResjTDnPIUMT3HcAdBGSaUZmd2yTkyLIpijeHHWp-ASc5ozgvGEZHRlLMt7mMbPuJVo52d3vcnbr9cfwEoZV9j?type=png" />
                </div>

                <p>By merging states S3 and S4 into S2, we reduce the number of transitions, improving performance.</p>

                <h5>Real-World Applications</h5>
                <ul>
                    <li><strong>GCC & Clang Compilers:</strong> Optimize lexical analysis using minimized DFAs.</li>
                    <li><strong>Regex Engines:</strong> Minimize finite automata for faster pattern matching.</li>
                    <li><strong>Security Tools:</strong> Use DFA minimization for efficient malware detection.</li>
                </ul>

                <h4>Implementing Just-In-Time (JIT) Lexing for Performance</h4>
                <p>Just-In-Time (JIT) lexing is an on-demand tokenization technique where lexing occurs only when required, rather than preprocessing the entire source code in advance.</p>

                <h5>Why JIT Lexing is Useful</h5>
                <ul>
                    <li>Reduces preprocessing time by analyzing only executed code paths.</li>
                    <li>Improves memory efficiency by avoiding unnecessary token storage.</li>
                    <li>Essential for dynamic and interpreted languages.</li>
                </ul>

                <h5>How JIT Lexing Works</h5>
                <ul>
                    <li><strong>Traditional Lexer:</strong> Scans and tokenizes the entire source file before parsing.</li>
                    <li><strong>JIT Lexer:</strong> Tokenizes only the portion being executed.</li>
                </ul>

                <h5>Example: Traditional vs. JIT Lexing</h5>

                <h5>Traditional Lexing (Static Compilation):</h5>
                <pre><code class="language-c">
int main() {
    int x = 10;
    if (x > 5) {
        printf("Hello");
    }
}
</code></pre>
                <p>The traditional lexer scans all tokens before parsing begins.</p>

                <h5>JIT Lexing (Dynamic Interpretation):</h5>
                <pre><code class="language-python">
if condition():
    execute()
</code></pre>
                <p>JIT lexing only tokenizes the if-statement when `condition()` is called.</p>

                <h5>Real-World Implementations</h5>
                <ul>
                    <li><strong>JavaScript V8 Engine (Google Chrome):</strong> Uses JIT lexing for optimized script execution.</li>
                    <li><strong>Just-in-Time Compilers (JITs):</strong> Dynamically analyze source code.</li>
                    <li><strong>Lua Interpreters:</strong> Perform JIT lexing for efficiency.</li>
                </ul>

            </article>



        </main>

        <script> copyright("all"); </script>

    </body>

</html>