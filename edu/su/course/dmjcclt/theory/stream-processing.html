<!-------------------------- © 2007 - present, dmj.one and contributors. ----------------------------------
   Part of the dmjone project. Licensed under the GNU AGPL. Provided as-is, without warranty of any kind. 
-------------------- Redistribution and modifications must retain this notice. --------------------------->


<!DOCTYPE html>
<!--[if lte 8]><html class="pre-ie9" lang="en"><![endif]-->
<!--[if gte IE 9]><!-->
<html lang="en">
    <!--<![endif]-->

    <head>
        <script src="/js/edu_su_common.js"></script>
        <noscript>
            <style>
                html,
                body {
                    margin: 0;
                    overflow: hidden;
                }
            </style>
            <iframe src="/frame_noscript.html" style="width:100%;height:100vh;border:none;display:block"></iframe>
        </noscript>

        <title>Stream Processing - DMJCCLT - dmj.one</title>
        <meta name="description" content="Know about Stream Processing and their applications in Distributed Systems.">

        <meta property="og:image" content="/logo.png">
        <meta property="og:type" content="article">

        <meta name="twitter:card" content="summary">
        <meta name="twitter:site" content="@divyamohan1993">
        <meta name="twitter:creator" content="@divyamohan1993">
        <meta name="twitter:image" content="/logo.png">

        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />

        <meta name="author" content="Divya Mohan">
        <meta name="robots" content="index, follow">

        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js" integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js" integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                renderMathInElement(document.body, {
                    // customised options
                    // • auto-render specific keys, e.g.:
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                        { left: '\\(', right: '\\)', display: false },
                        { left: '\\[', right: '\\]', display: true }
                    ],
                    // • rendering keys, e.g.:
                    throwOnError: false
                });
            });
        </script> -->

        <!-- <style>
            main ul {
                list-style-type: none;
                padding: 0;
                margin: 0;
            }

            main ul li {
                margin: 0;
                padding: 0;
            }
        </style> -->

    </head>

    <body>

        <script> header_author("dm", "lakshika"); </script>

        <main>
            <article class="agen-tableofcontents">
                <h2 class="text-center">
                    Stream Processing
                </h2>
                <div class="d-none contentdate">2024, December 31</div>
            </article>

            <article>
                <h3>1. Introduction to Stream Processing</h3>

                <h4>1.1 What is Stream Processing?</h4>
                <p>Stream processing is the real-time computation of continuously flowing data. Unlike traditional batch processing, which processes stored datasets after collection, stream processing handles data as it arrives. Each data point, called a <strong>tuple</strong>, is processed without waiting for the entire dataset to accumulate.</p>

                <h4>1.2 Why Stream Processing?</h4>
                <p>Modern applications demand immediate insights for decision-making. Scenarios like social media trends, online fraud detection, and live traffic analytics require near-instantaneous responses:</p>
                <ul>
                    <li><strong>Social Media Analytics:</strong> Platforms like Twitter need real-time tracking of hashtags and user activity to surface trending topics.</li>
                    <li><strong>Website Statistics:</strong> Services like Google Analytics rely on continuous data collection to provide live user metrics.</li>
                    <li><strong>Intrusion Detection:</strong> Datacenters use real-time analysis of logs to detect and mitigate security threats.</li>
                </ul>

                <h4>1.3 How Does Stream Processing Work?</h4>
                <p>Stream processing systems operate by ingesting data streams, performing computations, and producing actionable results with minimal latency. Key elements include:</p>
                <ul>
                    <li><strong>Continuous Data Flow:</strong> Data flows in real-time, enabling immediate processing.</li>
                    <li><strong>Low Latency:</strong> Operations occur in milliseconds or seconds, ensuring timely insights.</li>
                    <li><strong>Scalable Architecture:</strong> Systems handle large data volumes by distributing computation across nodes.</li>
                </ul>

            </article>

            <article>
                <h3>2. Key Concepts in Stream Processing</h3>

                <h4>2.1 Data Flow in Streams</h4>
                <p>Data in stream processing systems is modeled as continuous sequences called <strong>streams</strong>. Each stream consists of individual units of data known as <strong>tuples</strong>. These tuples are small, self-contained structures that encode essential information for processing:</p>
                <ul>
                    <li><strong>What:</strong> A tuple represents a single record, e.g., a tweet or a web access log.</li>
                    <li><strong>Why:</strong> Tuples are atomic units that ensure modularity and enable independent processing.</li>
                    <li><strong>How:</strong> Systems ingest tuples as they are generated (e.g., a tweet posted or a web page accessed), process them in real time, and pass results downstream.</li>
                </ul>
                <p>Example:
                <ul>
                    <li><code>{"user": "Miley Cyrus", "tweet": "Hey! Here's my new song!"}</code>: Captures real-time user activity on social platforms.</li>
                    <li><code>{"url": "coursera.org", "ip": "101.201.301.401", "timestamp": "4/4/2014, 10:35:40"}</code>: Represents a record in website analytics.</li>
                </ul>
                </p>

                <h4>2.2 Core Components</h4>

                <h5>2.2.1 Spout</h5>
                <p>
                    <strong>What:</strong> A spout is the entry point for tuples into the system.
                    <strong>Why:</strong> It abstracts data sources (e.g., APIs, databases) to provide a unified data stream to the processing pipeline.
                    <strong>How:</strong> A spout reads external data (e.g., tweets from Twitter’s API or logs from a database) and emits tuples into the system for processing.
                </p>

                <h5>2.2.2 Bolt</h5>
                <p>
                    <strong>What:</strong> Bolts are the processing units in stream processing.
                    <strong>Why:</strong> They handle all computation, such as filtering, transformations, and aggregations.
                    <strong>How:</strong> A bolt takes tuples as input, applies operations (e.g., filtering tweets with a specific hashtag), and outputs processed tuples for downstream components.
                </p>

                <h5>2.2.3 Stream</h5>
                <p>
                    <strong>What:</strong> A stream is a continuous flow of tuples.
                    <strong>Why:</strong> It acts as the communication medium between spouts and bolts.
                    <strong>How:</strong> Streams transport tuples, ensuring ordered delivery and reliability, which are crucial for accurate computation.
                </p>

                <h5>2.2.4 Topology</h5>
                <p>
                    <strong>What:</strong> A topology is the overall blueprint of the data processing logic, represented as a directed graph.
                    <strong>Why:</strong> It defines the relationships and dependencies between spouts and bolts.
                    <strong>How:</strong> Developers design topologies to implement specific use cases, connecting spouts to bolts in sequence or parallel for efficiency.
                </p>
            </article>

            <article>
                <h3>3. Apache Storm for Stream Processing</h3>

                <article>
                    <h4>3.1 Why Apache Storm?</h4>

                    <h5>What is Apache Storm?</h5>
                    <p>Apache Storm is a distributed real-time stream processing framework. It enables applications to process and analyze large volumes of streaming data with minimal latency.</p>

                    <h5>Why Apache Storm?</h5>
                    <p>Apache Storm is uniquely suited for stream processing due to its key features:</p>
                    <ul>
                        <li><strong>Real-Time Processing:</strong> Processes data as it arrives, providing insights within seconds or milliseconds.</li>
                        <li><strong>Fault Tolerance:</strong> Automatically detects and recovers from component failures without data loss.</li>
                        <li><strong>High Throughput:</strong> Handles millions of tuples per second by parallelizing processing across nodes.</li>
                        <li><strong>Scalability:</strong> Supports horizontal scaling to accommodate increasing data volumes.</li>
                        <li><strong>Flexibility:</strong> Works with multiple programming languages and integrates seamlessly with existing data ecosystems.</li>
                    </ul>

                    <h5>How is Apache Storm Used?</h5>
                    <p>Storm powers diverse applications by enabling real-time data processing pipelines. Examples include:</p>
                    <ul>
                        <li><strong>Twitter:</strong> Delivers personalized content and real-time search by analyzing streams of user activity.</li>
                        <li><strong>Flipboard:</strong> Curates and generates custom content feeds by processing live updates from publishers.</li>
                        <li><strong>Weather Channel:</strong> Processes continuous weather data streams for immediate forecasting and alert systems.</li>
                    </ul>
                    <p>Storm's architecture allows these systems to handle dynamic, high-speed data flows while maintaining reliability and performance.</p>
                </article>

                <article>
                    <h4>3.2 Storm Components</h4>

                    <h5>Tuple</h5>
                    <p>
                        <strong>What:</strong> A tuple is the fundamental unit of data in Apache Storm, structured as an ordered list of elements. Each tuple encapsulates specific information for processing.
                        <strong>Why:</strong> Tuples enable modular and flexible data representation, supporting a variety of formats (e.g., strings, integers, or complex objects).
                        <strong>How:</strong> Tuples are created and manipulated in spouts and bolts. They flow through the system, carrying data from one processing step to another.
                    </p>
                    <pre><code class="language-java">
// Example: A tuple representing a tweet
Tuple tuple = new Tuple("user", "Miley Cyrus", "tweet", "Here's my new song!");
</code></pre>

                    <h5>Spout</h5>
                    <p>
                        <strong>What:</strong> Spouts are data producers that generate and emit tuples into the Storm topology.
                        <strong>Why:</strong> They abstract external data sources, allowing seamless integration with databases, APIs, or message queues.
                        <strong>How:</strong> Spouts can be configured to pull data continuously (streaming mode) or in batches (batched mode). Each spout may emit multiple streams of tuples.
                    </p>
                    <pre><code class="language-java">
// Example: A spout emitting tweets
public class TwitterSpout extends BaseRichSpout {
    public void nextTuple() {
        emit(new Values("user", "Miley Cyrus", "tweet", "Here's my new song!"));
    }
}
</code></pre>

                    <h5>Bolt</h5>
                    <p>
                        <strong>What:</strong> Bolts are processing units that consume tuples, apply transformations, and emit processed tuples.
                        <strong>Why:</strong> They enable operations such as filtering irrelevant data, joining streams, aggregating statistics, or applying custom functions.
                        <strong>How:</strong> Bolts process input streams and produce output streams, chaining together to form a complete processing pipeline.
                    </p>
                    <pre><code class="language-java">
// Example: A bolt filtering tweets with a specific hashtag
public class HashtagFilterBolt extends BaseRichBolt {
    public void execute(Tuple input) {
        if (input.getStringByField("tweet").contains("#Music")) {
            collector.emit(input);
        }
    }
}
</code></pre>

                    <h5>Topology</h5>
                    <p>
                        <strong>What:</strong> A topology is the complete processing workflow, represented as a directed acyclic graph (DAG) of spouts and bolts.
                        <strong>Why:</strong> It defines the logical flow of data, enabling precise orchestration of computations.
                        <strong>How:</strong> Developers design topologies by connecting spouts and bolts, specifying parallelism and dependencies. The topology runs continuously, processing data streams indefinitely.
                    </p>
                    <pre><code class="language-java">
// Example: Defining a topology
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("tweet-spout", new TwitterSpout());
builder.setBolt("hashtag-filter", new HashtagFilterBolt())
       .shuffleGrouping("tweet-spout");
StormSubmitter.submitTopology("TwitterTopology", config, builder.createTopology());
</code></pre>
                </article>

                <article>
                    <h4>3.3 Processing Strategies</h4>

                    <h5>Optimizing Bolt Parallelism</h5>
                    <p>
                        Stream processing systems like Apache Storm rely on bolt parallelism to handle high-throughput data. Bolts are split into multiple tasks, and incoming tuples are routed to these tasks based on a specified <strong>grouping strategy</strong>. These strategies ensure efficient load balancing and data routing for varied use cases.
                    </p>

                    <h5>Shuffle Grouping</h5>
                    <p>
                        <strong>What:</strong> Tuples are distributed evenly across all bolt tasks in a round-robin fashion.
                        <strong>Why:</strong> Ensures uniform load distribution, preventing any single task from becoming a bottleneck.
                        <strong>How:</strong> Each tuple is sent to the next task in sequence. This approach is suitable when tuples are independent and do not need field-specific processing.
                    </p>
                    <pre><code class="language-java">
// Example: Configuring Shuffle Grouping
builder.setBolt("processor-bolt", new ProcessorBolt())
       .shuffleGrouping("source-spout");
</code></pre>

                    <h5>Fields Grouping</h5>
                    <p>
                        <strong>What:</strong> Tuples are routed to tasks based on a subset of their fields.
                        <strong>Why:</strong> Allows grouping of tuples with shared attributes, such as user IDs or product categories, ensuring related data is processed together.
                        <strong>How:</strong> A hash function is applied to the specified field(s) to determine the target task.
                    </p>
                    <pre><code class="language-java">
// Example: Configuring Fields Grouping
builder.setBolt("user-aggregator-bolt", new UserAggregatorBolt())
       .fieldsGrouping("source-spout", new Fields("userID"));
</code></pre>

                    <h5>All Grouping</h5>
                    <p>
                        <strong>What:</strong> All tuples are broadcasted to all tasks of a bolt.
                        <strong>Why:</strong> Useful for operations requiring a global view of data, such as joins or state synchronization across tasks.
                        <strong>How:</strong> Every task receives a copy of each tuple, ensuring comprehensive processing.
                    </p>
                    <pre><code class="language-java">
// Example: Configuring All Grouping
builder.setBolt("join-bolt", new JoinBolt())
       .allGrouping("source-spout");
</code></pre>

                    <h5>Choosing the Right Grouping Strategy</h5>
                    <p>
                        The choice of grouping strategy depends on the nature of the data and the desired computation:
                    <ul>
                        <li><strong>Shuffle Grouping:</strong> For stateless operations or independent processing.</li>
                        <li><strong>Fields Grouping:</strong> When related data needs to be processed together, e.g., user-based aggregation.</li>
                        <li><strong>All Grouping:</strong> For tasks requiring awareness of all incoming tuples, such as global joins or broadcasts.</li>
                    </ul>
                    </p>
                </article>
            </article>

            <article>
                <h3>4. Fault Tolerance in Stream Processing</h3>

                <h4>4.1 Handling Failures</h4>
                <p>
                    <strong>What:</strong> Fault tolerance in stream processing ensures that data processing continues reliably even when failures occur in the system.
                    <strong>Why:</strong> Failures, such as node crashes or network disruptions, can lead to data loss or incomplete processing if not handled effectively. Storm employs <strong>anchoring</strong> to mitigate these risks.
                    <strong>How:</strong> Anchoring links output tuples to their corresponding input tuples, creating a dependency chain. If an output tuple is not successfully processed within a specified timeout, the linked input tuple is replayed, ensuring no data is lost.
                </p>
                <pre><code class="language-java">
// Example of Anchoring
collector.emit(inputTuple, new Values("processedData"));
</code></pre>

                <h4>4.2 API for Fault Management</h4>
                <p>Storm's <code>OutputCollector</code> API offers methods to manage tuple processing states effectively:</p>

                <h5>4.2.1 <code>emit(tuple, output)</code></h5>
                <p>
                    <strong>What:</strong> Emits an output tuple, optionally anchored to an input tuple.
                    <strong>Why:</strong> Anchoring ensures traceability and enables tuple replay in case of failure.
                    <strong>How:</strong> Use the <code>emit</code> method to produce new tuples and establish anchoring for fault tolerance.
                </p>
                <pre><code class="language-java">
// Example: Emitting with Anchoring
collector.emit(inputTuple, new Values("transformedData"));
</code></pre>

                <h5>4.2.2 <code>ack(tuple)</code></h5>
                <p>
                    <strong>What:</strong> Acknowledges that a tuple has been successfully processed.
                    <strong>Why:</strong> Ensures efficient memory management by removing processed tuples from the system.
                    <strong>How:</strong> Call <code>ack</code> after completing tuple processing.
                </p>
                <pre><code class="language-java">
// Example: Acknowledging a Tuple
collector.ack(inputTuple);
</code></pre>

                <h5>4.2.3 <code>fail(tuple)</code></h5>
                <p>
                    <strong>What:</strong> Marks a tuple as failed and triggers its replay.
                    <strong>Why:</strong> Prevents data loss by reprocessing tuples that encountered errors.
                    <strong>How:</strong> Call <code>fail</code> in case of exceptions or processing errors.
                </p>
                <pre><code class="language-java">
// Example: Marking a Tuple as Failed
collector.fail(inputTuple);
</code></pre>

                <h4>Key Best Practices</h4>
                <ul>
                    <li><strong>Explicit State Management:</strong> Always use <code>ack</code> or <code>fail</code> for every tuple to prevent memory leaks.</li>
                    <li><strong>Timeout Configuration:</strong> Configure appropriate timeouts for tuple processing to balance fault tolerance and performance.</li>
                    <li><strong>Efficient Anchoring:</strong> Anchor only when necessary to avoid unnecessary overhead.</li>
                </ul>
                <p>Storm's fault tolerance mechanisms ensure reliability and robustness in real-time data processing pipelines, even under adverse conditions.</p>
            </article>

            <article>
                <h3>5. Summary</h3>
                <p>
                    Stream processing systems, exemplified by Apache Storm, are pivotal for real-time data processing. These systems address the growing need for low-latency, high-throughput analysis in diverse applications. Unlike batch processing, which processes data after accumulation, stream processing transforms data as it flows, enabling immediate insights and actions.
                </p>
                <p>
                    <strong>Key Capabilities:</strong>
                </p>
                <ul>
                    <li><strong>Real-Time Processing:</strong> Handles continuous data streams, offering near-instantaneous results.</li>
                    <li><strong>High Throughput:</strong> Efficiently processes vast volumes of data by leveraging distributed architectures and parallelism.</li>
                    <li><strong>Fault Tolerance:</strong> Ensures reliability through mechanisms like anchoring and tuple replay, maintaining data integrity even during failures.</li>
                </ul>
                <p>
                    <strong>Core Components:</strong> The system's architecture revolves around:
                </p>
                <ul>
                    <li><strong>Spouts:</strong> Sources of data, interfacing with external systems.</li>
                    <li><strong>Bolts:</strong> Units of computation that apply operations like filtering, joining, and aggregation.</li>
                    <li><strong>Topologies:</strong> Directed graphs defining the logical flow of data between spouts and bolts.</li>
                </ul>
                <p>
                    <strong>Applications:</strong> Apache Storm powers diverse real-world use cases, such as:
                </p>
                <ul>
                    <li><strong>Social Media Analytics:</strong> Real-time trend tracking and personalization (e.g., Twitter).</li>
                    <li><strong>Website Analytics:</strong> Live user metrics and behavior analysis (e.g., Google Analytics).</li>
                    <li><strong>Security Systems:</strong> Intrusion detection in datacenters.</li>
                </ul>
                <p>
                    Apache Storm demonstrates how modern stream processing systems address the inherent limitations of batch processing frameworks, enabling rapid decision-making and operational efficiency in data-intensive environments.
                </p>
            </article>




        </main>

        <script> copyright("all"); </script>

    </body>

</html>