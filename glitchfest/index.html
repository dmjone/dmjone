<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Hackathon Challenges</title>
        <!-- Google Fonts -->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&family=Roboto:wght@400;500&display=swap" rel="stylesheet" />
        <!-- Bootstrap CSS CDN (optional for responsiveness) -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" />
        <style>
            /* Base styles for screen */
            body {
                background-color: #ffffff;
                color: #000000;
                font-family: 'Roboto', sans-serif;
                padding: 20px;
                line-height: 1.5;
            }

            h1,
            h2 {
                font-family: 'Merriweather', serif;                
                margin-top: 20px;
                margin-bottom: 20px;
            }

            p,
            ul {
                line-height: 1.6;
                margin-bottom: 15px;
            }

            ul {
                padding-left: 20px;
            }

            .container {
                max-width: 900px;
                margin: auto;
            }

            /* Hide watermark on screen */
            .watermark {
                display: none;
            }

            .d-print-none {
                display: block;
            }

            section h2 {
                text-align: center;
            }

            /* Print styles */
            @media print {
                @page {
                    margin: 1in;
                }

                * {
                    padding: 0;
                    margin: 0;
                    font-family: 'Times New Roman', Times, serif;                    
                }

                h1, h2 {
                    font-size: 14pt;
                }

                .d-print-none {
                    display: none;
                }

                body {
                    position: relative;
                    text-align: justify;
                    font-family: 'Times New Roman', Times, serif;
                    font-size: 12pt;
                    line-height: 1.5;
                }

                /* Watermark styling for print only */
                .watermark {
                    display: block;
                    position: fixed;
                    top: 50%;
                    left: 50%;
                    transform: translate(-50%, -50%);
                    opacity: 0.1;
                    z-index: -1;
                    max-width: 200px;
                    width: 100%;
                }

                /* Break page after each section */
                section {
                    page-break-after: always;
                }
            }
        </style>
    </head>

    <body>
        <!-- Watermark for print only -->
        <img src="/logo.png" alt="Watermark" class="watermark" />

        <div class="container">
            <h1 class="d-print-none text-center">GlitchFest 2025</h1>

            <section id="problem1">
                <h2>Problem 1: Autonomous Penetration Testing Agent</h2>
                <p>
                    Modern enterprises need continuous security testing, but manual penetration testing is infrequent and labor-intensive. An AI-driven agent that autonomously probes systems for vulnerabilities could dramatically improve security testing coverage. This agent should intelligently plan and execute attacks (e.g. SQL injection, XSS, or port exploitation) on a provided test environment without human intervention, learning and adapting like a human pentester.
                </p>
                <ul>
                    <li><strong>Objective:</strong> Develop an AI agent that can <strong>discover and exploit security vulnerabilities</strong> in a controlled target system autonomously. The agent should map the target, identify potential weaknesses, attempt exploits, and report findings.</li>
                    <li>
                        <strong>Constraints:</strong>
                        <ul>
                            <li>Operate <strong>within a sandboxed or simulated environment</strong> provided for testing (no external network scanning).</li>
                            <li>Use adaptive strategies (e.g. reinforcement learning or logical reasoning) to avoid simple brute-force; the agent should <strong>learn from failed attempts</strong>.</li>
                            <li>Ensure safety controls to <strong>prevent damage</strong> beyond the test scope (do not crash services unintentionally).</li>
                        </ul>
                    </li>
                    <li><strong>Expected Output:</strong> A demonstration of the agent attacking a vulnerable application and a detailed <strong>report of discovered vulnerabilities</strong> (e.g. type of flaw, how it was exploited). Success is measured by the agent’s ability to find <strong>at least one real vulnerability</strong> and provide reproducible steps or payloads, all within the 24-hour window.</li>
                </ul>
            </section>

            <section id="problem2">
                <h2>Problem 2: Real-Time Deepfake Impersonation Detection</h2>
                <p>
                    With the rise of AI-generated voices and videos, attackers can impersonate CEOs, officials, or loved ones in real-time calls to defraud or mislead. There is an urgent need for a system that can instantly verify whether a voice or video feed is genuine or a deepfake. This solution would protect against voice-phishing and video deepfake scams by analyzing subtle artifacts in audio/visual streams on the fly.
                </p>
                <ul>
                    <li><strong>Objective:</strong> Create an AI-powered tool that <strong>monitors live audio and/or video calls</strong> and <strong>flags impersonation attempts</strong> in real time. The system should analyze incoming speech (and facial video if applicable) to detect signs of deepfake generation or voice cloning.</li>
                    <li>
                        <strong>Constraints:</strong>
                        <ul>
                            <li><strong>Low latency:</strong> Detection must occur within seconds so that a fraudulent call can be interrupted immediately (processing per frame or audio chunk in real-time).</li>
                            <li><strong>High accuracy:</strong> Minimize false alarms – the model should reliably distinguish authentic voices/faces from AI-generated ones, even in varied network conditions or audio quality.</li>
                            <li>Should work for <strong>unseen speakers</strong> (no prior recording of the victim’s voice required) by recognizing synthetic artifacts (e.g. odd spectral patterns, lip-sync mismatches) rather than just matching to known samples.</li>
                        </ul>
                    </li>
                    <li><strong>Expected Output:</strong> A running prototype that can ingest a live stream (or simulate one) and produce an <strong>alert</strong> or confidence score whenever a segment is suspected to be a deepfake. Teams should demonstrate the system on example scenarios (e.g. a cloned voice trying to authorize a transaction) and show it correctly identifying the fake in real-time, with a brief explanation of the indicators used.</li>
                </ul>
            </section>

            <section id="problem3">
                <h2>Problem 3: Federated IoT Intrusion Detection System</h2>
                <p>
                    In IoT networks (smart homes, industrial sensors, etc.), each device generates data that could indicate cyber-attacks or anomalies. However, privacy and bandwidth limits mean raw data cannot simply be sent to a central server for analysis. The challenge is to detect coordinated attacks (like a botnet infecting many devices) using <strong>federated learning</strong> – where each device learns from its own data and only shares minimal insights or model updates.
                </p>
                <ul>
                    <li><strong>Objective:</strong> Build a <strong>distributed anomaly detection system</strong> for IoT devices that uses federated machine learning to identify cyber threats. Each device will locally train/update a lightweight model on its own sensor/network data, and a central aggregator will combine these updates to improve a global model detecting network intrusions or device anomalies.</li>
                    <li>
                        <strong>Constraints:</strong>
                        <ul>
                            <li><strong>No raw data sharing:</strong> Devices may only share model parameters or alert signals, preserving sensitive data privacy. The design must prevent leakage of private information through the shared updates.</li>
                            <li>The solution should handle <strong>heterogeneous devices</strong> (different data types or frequencies) and be robust to some devices going offline. It must be efficient enough to run on limited hardware (e.g. a Raspberry Pi).</li>
                            <li>Provide a strategy to deal with concept drift over time (devices learning new patterns) and <strong>malicious participants</strong> (if an infected device tries to poison the learning process).</li>
                        </ul>
                    </li>
                    <li><strong>Expected Output:</strong> A working proof-of-concept where multiple simulated IoT nodes run an intrusion detection model locally and a server periodically aggregates their learning. The final system should <strong>detect an orchestrated attack</strong> (for example, a malware outbreak across devices) that no single device could recognize in isolation. Teams should present detection results (e.g. anomaly scores or alerts) before and after federated aggregation, demonstrating improved threat detection due to collaboration.</li>
                </ul>
            </section>

            <section id="problem4">
                <h2>Problem 4: AI-Assisted Automated Vulnerability Patch Generation</h2>
                <p>
                    Software vulnerabilities often remain unpatched for weeks or months, leaving systems at risk. The goal is an AI system that can not only identify a security bug in code but also <strong>generate a patch</strong> automatically. This requires understanding the code context and crafting a fix that addresses the flaw without breaking functionality – a task that is currently unsolved and usually requires human expertise.
                </p>
                <ul>
                    <li><strong>Objective:</strong> Develop a solution that <strong>automatically generates a security patch</strong> for a given piece of vulnerable code. For example, if provided with a function containing a buffer overflow or an SQL injection flaw, the system should output a revised version of the code that fixes the issue.</li>
                    <li>
                        <strong>Constraints:</strong>
                        <ul>
                            <li>The tool may use static analysis or AI (such as code analysis with an LLM) to locate the vulnerability, but the patch suggestion must be produced with <strong>minimal human intervention</strong> (the team can fine-tune an AI model or create rules, but the final fix should be AI-derived).</li>
                            <li><strong>Preserve functionality:</strong> The patched code should still perform the original intended task (aside from the security issue). Teams should validate this with given test cases or by showing that intended inputs still produce correct outputs after patching.</li>
                            <li>Ensure the patch actually <strong>eliminates the vulnerability</strong> – for instance, if the original bug was an unchecked input, the fix might introduce proper validation or sanitization. The solution should consider edge cases (avoiding regressions or new issues).</li>
                        </ul>
                    </li>
                    <li><strong>Expected Output:</strong> A demonstration on one or more examples where the system takes vulnerable code as input and outputs a patched version. This should include a before-and-after comparison: the original code exploit (or failing security test) vs. the patched code resisting the exploit and passing all tests. The deliverables include the <strong>automated patch diff</strong> and a brief explanation of the fix applied for each vulnerability addressed.</li>
                </ul>
            </section>

            <section id="problem5">
                <h2>Problem 5: Adaptive Deceptive Honeypot System</h2>
                <p>
                    Honeypots are decoy systems used to lure attackers, but traditional honeypots have static configurations that skilled adversaries can eventually recognize. This problem asks for a next-generation honeypot that <strong>dynamically adapts its behavior</strong> using AI, making it indistinguishable from a real system. As an attacker interacts, the honeypot should intelligently adjust responses and even introduce fake data or changes to keep the attacker engaged and unaware.
                </p>
                <ul>
                    <li><strong>Objective:</strong> Create an <strong>AI-driven honeypot</strong> server that convincingly mimics a real system (such as a web application, IoT device, or database) and adapts in real time to attacker tactics. The system should log all attacker interactions for analysis, while altering its own behavior or data to continue the deception.</li>
                    <li>
                        <strong>Constraints:</strong>
                        <ul>
                            <li><strong>Dynamic response generation:</strong> Instead of fixed scripts, use AI or rule-based logic to generate believable outputs (error messages, file listings, sensor readings, etc.) based on the attacker’s input. For example, if an attacker queries for a specific file, the honeypot can “on-the-fly” create a fake file structure rather than returning a static preset list.</li>
                            <li>The honeypot must avoid obvious fingerprints. It should <strong>simulate realistic latency, banners, and data</strong> so that it closely resembles a live target. Any adaptive changes (like suddenly opening a new port or changing a protocol response) should have plausible reasons (perhaps simulating a user or cron job on the system).</li>
                            <li><strong>Resilience:</strong> The system should handle multi-step attack patterns and not crash or reveal itself even if the attacker tries unusual or unexpected inputs. Security of the honeypot itself is crucial (the attacker should not be able to easily break the honeypot mechanism).</li>
                        </ul>
                    </li>
                    <li><strong>Expected Output:</strong> A live demonstration of the honeypot in action against a simulated attack. Teams should show a sequence of attacker actions (e.g. scanning ports, attempting exploits, requesting data) and how the honeypot responds <strong>convincingly and adaptively</strong> at each step. Provide the <strong>attack log</strong> highlighting how the honeypot’s adaptive responses kept the attacker engaged. Success is indicated by the attacker (or simulated attacker script) treating the honeypot as a normal system throughout the interaction, allowing the defenders to collect rich telemetry.</li>
                </ul>
            </section>
        </div>
    </body>

</html>